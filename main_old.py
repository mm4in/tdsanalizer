#!/usr/bin/env python3
"""
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø
‚úÖ –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ü–†–û–î–í–ò–ù–£–¢–´–• –ú–û–î–£–õ–ï–ô
‚úÖ –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–ê–†–°–ï–† –í–°–ï–• –ü–û–õ–ï–ô  
‚úÖ LTF/HTF –†–ê–ó–î–ï–õ–ï–ù–ò–ï
‚úÖ –§–ê–ó–û–í–´–ô –ê–ù–ê–õ–ò–ó
‚úÖ VETO –°–ò–°–¢–ï–ú–ê

–ò–°–ü–†–ê–í–õ–Ø–ï–¢ –ö–†–ò–¢–ò–ß–ï–°–ö–£–Æ –û–®–ò–ë–ö–£: —Å–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è, 
–∞ –Ω–µ —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–≤–µ—á–µ–π!
"""

import pandas as pd
import numpy as np
import re
import yaml
import json
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve
import matplotlib.pyplot as plt

# –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–º–ø–æ—Ä—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
try:
    from advanced_log_parser import AdvancedLogParser
    from parser_integration import ParserIntegration
    from ltf_htf_analyzer import LTFHTFAnalyzer
    from scoring_api import ScoringAPI
    from enhanced_events_analyzer import EnhancedEventsAnalyzer
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
    print("üí° –†–∞–±–æ—Ç–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é")
    ADVANCED_MODULES_AVAILABLE = False


class FinancialLogAnalyzer:
    """
    –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –í–°–ï–• –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
    
    –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø:
    ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç AdvancedLogParser –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
    ‚úÖ –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è (nw, ef, as, vc, ze...)
    ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
    ‚úÖ –§–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏–π
    ‚úÖ VETO —Å–∏—Å—Ç–µ–º–∞ —Å—Ç–æ–ø-–ø–æ–ª–µ–π
    ‚úÖ Data-driven –ø–æ–¥—Ö–æ–¥ –±–µ–∑ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π
    """
    
    def __init__(self, config_path="config.yaml"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π"""
        self.config = self._load_config(config_path)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
        if ADVANCED_MODULES_AVAILABLE:
            self.advanced_parser = AdvancedLogParser()
            self.parser_integration = ParserIntegration(self)
            self.ltf_htf_analyzer = LTFHTFAnalyzer()
            self.enhanced_events = EnhancedEventsAnalyzer()
            print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥—É–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
        else:
            print("‚ö†Ô∏è –†–∞–±–æ—Ç–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        self.parsed_data = None
        self.features = None
        self.targets = None
        self.events = None
        
        # –ù–û–í–û–ï: –î–∞–Ω–Ω—ã–µ –∏–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
        self.raw_parsing_data = None
        self.ltf_data = None
        self.htf_data = None
        self.ltf_results = None
        self.htf_results = None
        self.combined_features = None
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.correlation_results = None
        self.threshold_results = None
        self.scoring_system = None
        self.validation_results = None
        self.temporal_analysis = None

    def _load_config(self, config_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        default_config = {
            'analysis': {
                'min_accuracy': 0.7,
                'min_lift': 2.0,
                'enable_ltf_htf': True,
                'enable_advanced_events': True,
                'enable_veto_system': True,
                'enable_temporal_analysis': True
            },
            'thresholds': {
                'event_volatility': 0.8,
                'event_change': 0.8,
                'event_volume': 0.8
            }
        }
        
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                if user_config:
                    default_config.update(user_config)
        
        return default_config

    def parse_log_file(self, file_path):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ü–ê–†–°–ò–ù–ì —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        
        –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï:
        - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç AdvancedLogParser –≤–º–µ—Å—Ç–æ —Å—Ç–∞—Ä–æ–≥–æ _parse_line
        - –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è (nw, ef, as, vc, ze...)
        - –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        """
        print(f"üîç –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path}")
        
        if ADVANCED_MODULES_AVAILABLE:
            # –ù–û–í–´–ô –°–ü–û–°–û–ë: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
            print("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")
            
            # –ü–æ–ª–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö –ø–æ–ª–µ–π
            self.raw_parsing_data = self.advanced_parser.parse_log_file(file_path)
            
            if self.raw_parsing_data.empty:
                print("‚ùå –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–µ –∏–∑–≤–ª–µ–∫ –¥–∞–Ω–Ω—ã–µ")
                return self._fallback_parse_log_file(file_path)
            
            # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            integration_results = self.parser_integration.replace_old_parser(file_path)
            
            if integration_results:
                self.parsed_data = integration_results['full_data']
                self.ltf_data = integration_results['ltf_data']
                self.htf_data = integration_results['htf_data']
                
                print(f"‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω:")
                print(f"   üìä –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.parsed_data)}")
                print(f"   üîÑ LTF –∑–∞–ø–∏—Å–µ–π: {len(self.ltf_data) if self.ltf_data is not None else 0}")
                print(f"   üêå HTF –∑–∞–ø–∏—Å–µ–π: {len(self.htf_data) if self.htf_data is not None else 0}")
                print(f"   üéØ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {len(self.parsed_data.columns)}")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π
                self._verify_critical_fields_extraction()
                
                return self.parsed_data
            
        # Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –º–µ—Ç–æ–¥—É –µ—Å–ª–∏ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã
        return self._fallback_parse_log_file(file_path)

    def _verify_critical_fields_extraction(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π –∏–∑ –¢–ó"""
        critical_fields = ['nw2', 'ef2', 'as2', 'vc2', 'ze2', 'cvz2', 'maz2']
        found_fields = []
        
        for field in critical_fields:
            if field in self.parsed_data.columns:
                found_fields.append(field)
        
        if found_fields:
            print(f"üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–õ–Ø –ù–ê–ô–î–ï–ù–´: {found_fields}")
            print("‚úÖ –°–∏—Å—Ç–µ–º–∞ —Ç–µ–ø–µ—Ä—å –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ò–ù–î–ò–ö–ê–¢–û–†–´, –∞ –Ω–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ!")
        else:
            print("‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —Ñ–æ—Ä–º–∞—Ç –ª–æ–≥–∞")

    def _fallback_parse_log_file(self, file_path):
        """Fallback –∫ —Å—Ç–∞—Ä–æ–º—É –ø–∞—Ä—Å–µ—Ä—É (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        print(f"üîÑ Fallback –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path}")
        
        records = []
        
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    line = line.strip()
                    if not line or not line.startswith('['):
                        continue
                    
                    record = self._parse_line(line, line_num)
                    if record:
                        records.append(record)
                        
                except Exception as e:
                    print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–æ–∫–∏ {line_num}: {e}")
                    continue
        
        self.parsed_data = pd.DataFrame(records)
        print(f"‚úÖ Fallback: –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.parsed_data)} –∑–∞–ø–∏—Å–µ–π")
        
        return self.parsed_data
    
    def _parse_line(self, line, line_num):
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ (—Å–æ—Ö—Ä–∞–Ω–µ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)"""
        try:
            match = re.match(r'\[([^\]]+)\]:\s*(.+)', line)
            if not match:
                return None
            
            timestamp_str, data_str = match.groups()
            parts = data_str.split('|')
            
            if len(parts) < 6:
                return None
            
            record = {
                'log_timestamp': pd.to_datetime(timestamp_str),
                'log_type': parts[0],
                'event_name': parts[1],
                'timeframe': parts[2],
                'event_timestamp': parts[3],
                'line_number': line_num
            }
            
            candle_data, field_data = self._parse_candle_and_fields('|'.join(parts[4:]))
            record.update(candle_data)
            record.update(field_data)
            
            return record
            
        except Exception:
            return None

    def _parse_candle_and_fields(self, data_str):
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ (—É–ª—É—á—à–µ–Ω –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –±–æ–ª—å—à–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–ª–µ–π)"""
        candle_data = {}
        field_data = {}
        
        parts = data_str.split('|')
        
        # –°–≤–µ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        if len(parts) >= 6:
            candle_data.update({
                'color': parts[0],
                'price_change': self._parse_number(parts[1].replace('%', '')),
                'volume': self._parse_volume(parts[2]),
                'candle_type': parts[3],
                'completion': self._parse_number(parts[4].replace('%', '')),
                'movement_24h': self._parse_number(parts[5].replace('%_24h', '').replace('%', ''))
            })
        
        # OHLC –¥–∞–Ω–Ω—ã–µ
        remaining_data = '|'.join(parts[6:]) if len(parts) > 6 else ''
        ohlc_match = re.search(r'o:([\d.]+).*?h:([\d.]+).*?l:([\d.]+).*?c:([\d.]+)', remaining_data)
        if ohlc_match:
            candle_data.update({
                'open': float(ohlc_match.group(1)),
                'high': float(ohlc_match.group(2)),
                'low': float(ohlc_match.group(3)),
                'close': float(ohlc_match.group(4))
            })
        
        # Range
        rng_match = re.search(r'rng:([\d.]+)', remaining_data)
        if rng_match:
            candle_data['range'] = float(rng_match.group(1))
        
        # –£–õ–£–ß–®–ï–ù–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ –ø–æ–ª–µ–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        field_part = re.sub(r'o:[\d.]+.*?c:[\d.]+.*?rng:[\d.]+\|?', '', remaining_data)
        if field_part:
            # –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∞–∫ '|' —Ç–∞–∫ –∏ ',' —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
            field_items = field_part.replace('|', ',').split(',')
            for item in field_items:
                item = item.strip()
                if '-' in item and item:
                    field_parts = item.split('-', 1)
                    if len(field_parts) == 2:
                        field_name = field_parts[0].strip()
                        field_value = self._parse_field_value(field_parts[1])
                        if field_name:  # –ù–µ –ø—É—Å—Ç–æ–µ –∏–º—è
                            field_data[field_name] = field_value
        
        return candle_data, field_data

    def _parse_number(self, value_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        try:
            return float(value_str) if value_str.replace('.', '').replace('-', '').isdigit() else 0
        except:
            return 0
    
    def _parse_volume(self, volume_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ–±—ä–µ–º–∞"""
        try:
            volume_str = volume_str.upper()
            if 'K' in volume_str:
                return float(volume_str.replace('K', '')) * 1000
            elif 'M' in volume_str:
                return float(volume_str.replace('M', '')) * 1000000
            return float(volume_str)
        except:
            return 0
    
    def _parse_field_value(self, value_str):
        """–£–õ–£–ß–®–ï–ù–ù–´–ô –ø–∞—Ä—Å–∏–Ω–≥ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ–ª–µ–π"""
        try:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π (!! !!! –∏ —Ç.–¥.)
            clean_value = str(value_str).replace('%', '').replace('œÉ', '')
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ—Å–∫–ª–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞–∫–æ–≤
            if '!!!' in clean_value:
                return 3.0  # –û—á–µ–Ω—å —Å–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª
            elif '!!' in clean_value:
                return 2.0  # –°–∏–ª—å–Ω—ã–π —Å–∏–≥–Ω–∞–ª  
            elif '!' in clean_value:
                return 1.0  # –û–±—ã—á–Ω—ã–π —Å–∏–≥–Ω–∞–ª
            
            # –ß–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            clean_value = clean_value.replace('!', '')
            if clean_value.replace('.', '').replace('-', '').isdigit():
                return float(clean_value)
            
            return 0
        except:
            return 0

    def detect_market_events(self):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        """
        print("üéØ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π...")
        
        if self.parsed_data is None or len(self.parsed_data) == 0:
            return
        
        if ADVANCED_MODULES_AVAILABLE and self.config['analysis']['enable_advanced_events']:
            # –ù–û–í–´–ô –°–ü–û–°–û–ë: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π
            print("üöÄ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π...")
            
            try:
                # –ê–Ω–∞–ª–∏–∑ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ —Å–æ–±—ã—Ç–∏–π
                self.enhanced_events.events_data = self.parsed_data
                advanced_events = self.enhanced_events.analyze_practical_events()
                
                if advanced_events:
                    print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω")
                    # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ—Å–Ω–æ–≤–Ω—É—é —Å–∏—Å—Ç–µ–º—É
                    self._integrate_advanced_events(advanced_events)
                else:
                    print("‚ö†Ô∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ –≤–µ—Ä–Ω—É–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã")
                    self._fallback_detect_events()
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞: {e}")
                self._fallback_detect_events()
        else:
            # Fallback –∫ –±–∞–∑–æ–≤–æ–º—É –º–µ—Ç–æ–¥—É
            self._fallback_detect_events()
        
        return self.events

    def _integrate_advanced_events(self, advanced_events):
        """–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–±—ã—Ç–∏–π"""
        df = self.parsed_data.copy()
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–æ—Å—Ç–∞–≤–Ω–æ–≥–æ —Å–∫–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤
        event_score = 0
        event_types_found = 0
        
        for event_type, analysis in advanced_events.items():
            if analysis.get('count', 0) > 0:
                event_types_found += 1
                event_score += analysis.get('frequency', 0)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω–æ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞ —Å–æ–±—ã—Ç–∏–π
        if event_types_found > 0:
            df['is_event'] = (event_score > 0.1).astype(int)
            events_count = df['is_event'].sum()
            events_rate = events_count / len(df)
        else:
            df['is_event'] = 0
            events_count = 0
            events_rate = 0.0
        
        self.events = {
            'total_events': events_count,
            'event_rate': events_rate,
            'event_types': advanced_events,
            'advanced_analysis': True
        }
        
        self.parsed_data = df
        print(f"‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–æ {events_count} —Å–æ–±—ã—Ç–∏–π ({events_rate:.2%})")

    def _fallback_detect_events(self):
        """Fallback –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        print("üîÑ Fallback –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π...")
        
        df = self.parsed_data.copy()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        numeric_cols = ['high', 'low', 'open', 'close', 'volume', 'price_change']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
        
        # Data-driven –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏
        df['price_volatility'] = (df['high'] - df['low']) / df['close'] * 100
        df['price_change_abs'] = abs(df['price_change'])
        df['volume_log'] = np.log1p(df['volume'])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ infinity –∏ NaN
        for col in ['price_volatility', 'price_change_abs', 'volume_log']:
            df[col] = df[col].replace([np.inf, -np.inf], 0).fillna(0)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏ –¥–ª—è —Å–æ–±—ã—Ç–∏–π
        vol_threshold = df['price_volatility'].quantile(0.8)
        change_threshold = df['price_change_abs'].quantile(0.8)
        volume_threshold = df['volume_log'].quantile(0.8)
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
        df['is_event'] = (
            (df['price_volatility'] > vol_threshold) |
            (df['price_change_abs'] > change_threshold) |
            (df['volume_log'] > volume_threshold)
        ).astype(int)
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Å–æ–±—ã—Ç–∏–π
        events_count = df['is_event'].sum()
        events_rate = events_count / len(df)
        
        self.events = {
            'total_events': events_count,
            'event_rate': events_rate,
            'vol_threshold': vol_threshold,
            'change_threshold': change_threshold,
            'volume_threshold': volume_threshold,
            'advanced_analysis': False
        }
        
        self.parsed_data = df
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {events_count} —Å–æ–±—ã—Ç–∏–π ({events_rate:.2%})")

    def analyze_three_phases(self):
        """
        –ù–û–í–´–ô: –§–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
        –ü–û–î–ì–û–¢–û–í–ö–ê ‚Üí –ö–£–õ–¨–ú–ò–ù–ê–¶–ò–Ø ‚Üí –†–ê–ó–í–ò–¢–ò–ï
        """
        print("üé≠ –§–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏–π...")
        
        if ADVANCED_MODULES_AVAILABLE and self.config['analysis']['enable_temporal_analysis']:
            # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∞–∑
            try:
                if hasattr(self.enhanced_events, 'analyze_three_phases'):
                    self.temporal_analysis = self.enhanced_events.analyze_three_phases()
                    print("‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π —Ñ–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
                else:
                    self._basic_phase_analysis()
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Ñ–∞–∑–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
                self._basic_phase_analysis()
        else:
            self._basic_phase_analysis()
        
        return self.temporal_analysis

    def _basic_phase_analysis(self):
        """–ë–∞–∑–æ–≤—ã–π —Ñ–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑"""
        if self.parsed_data is None:
            return
        
        df = self.parsed_data.copy()
        
        # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ñ–∞–∑ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π
        self.temporal_analysis = {
            'preparation': {'duration_minutes': 30, 'frequency': 0.15},
            'culmination': {'duration_minutes': 5, 'frequency': 0.05},
            'development': {'duration_minutes': 45, 'frequency': 0.10}
        }
        
        print("‚úÖ –ë–∞–∑–æ–≤—ã–π —Ñ–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")

    def run_ltf_htf_analysis(self):
        """
        –ù–û–í–´–ô: –ó–∞–ø—É—Å–∫ LTF/HTF –∞–Ω–∞–ª–∏–∑–∞
        """
        print("üîó –ó–∞–ø—É—Å–∫ LTF/HTF –∞–Ω–∞–ª–∏–∑–∞...")
        
        if not ADVANCED_MODULES_AVAILABLE:
            print("‚ö†Ô∏è LTF/HTF –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω - –Ω—É–∂–Ω—ã –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥—É–ª–∏")
            return None
        
        if not self.config['analysis']['enable_ltf_htf']:
            print("üîò LTF/HTF –∞–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª—é—á–µ–Ω –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
            return None
        
        try:
            # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ LTF/HTF –∞–Ω–∞–ª–∏–∑–∞ —á–µ—Ä–µ–∑ –≥–æ—Ç–æ–≤—ã–π –º–æ–¥—É–ª—å
            if self.ltf_data is not None or self.htf_data is not None:
                # –î–∞–Ω–Ω—ã–µ —É–∂–µ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –≤ parse_log_file
                self.ltf_results = self.ltf_htf_analyzer.analyze_ltf_data(self.ltf_data)
                self.htf_results = self.ltf_htf_analyzer.analyze_htf_data(self.htf_data)
            else:
                # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º
                results = self.ltf_htf_analyzer.run_full_ltf_htf_analysis(
                    self.parsed_data if self.raw_parsing_data is None else self.raw_parsing_data
                )
                self.ltf_results = results.get('ltf_results')
                self.htf_results = results.get('htf_results')
            
            print("‚úÖ LTF/HTF –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω")
            return {'ltf': self.ltf_results, 'htf': self.htf_results}
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ LTF/HTF –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return None

    def build_feature_matrix(self):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π LTF/HTF
        """
        print("üèóÔ∏è –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ï –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        if self.parsed_data is None:
            return None
        
        # –ù–∞—á–∏–Ω–∞–µ–º —Å –±–∞–∑–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        df = self.parsed_data.copy()
        feature_columns = []
        
        # –ë–∞–∑–æ–≤—ã–µ —Ü–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
        price_features = ['open', 'high', 'low', 'close', 'range', 'price_change', 'volume']
        for feature in price_features:
            if feature in df.columns:
                df[feature] = pd.to_numeric(df[feature], errors='coerce').fillna(0)
                feature_columns.append(feature)
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π
        indicator_fields = []
        for col in df.columns:
            # –ü–æ–∏—Å–∫ –ø–æ–ª–µ–π –∏–∑ –¢–ó: nw, ef, as, vc, ze, cvz, maz –∏ –¥—Ä.
            if any(col.startswith(prefix) for prefix in [
                'nw', 'ef', 'as', 'vc', 'ze', 'cvz', 'maz', 'co', 'ro', 'mo', 
                'do', 'so', 'rz', 'mz', 'ciz', 'sz', 'dz', 'rd', 'md', 'cd'
            ]):
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)
                indicator_fields.append(col)
                feature_columns.append(col)
        
        print(f"üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–õ–Ø –ò–ó–í–õ–ï–ß–ï–ù–´: {len(indicator_fields)} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤")
        if indicator_fields:
            print(f"   –ü—Ä–∏–º–µ—Ä—ã: {indicator_fields[:10]}")
        
        # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –≤–∞–∂–Ω—ã—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        lag_features = []
        important_features = indicator_fields[:20]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 20 –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        for lag in [1, 2, 3]:
            for feature in important_features:
                if feature in df.columns:
                    lag_col = f"{feature}_lag_{lag}"
                    df[lag_col] = df[feature].shift(lag).fillna(0)
                    lag_features.append(lag_col)
        
        feature_columns.extend(lag_features)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è LTF/HTF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if ADVANCED_MODULES_AVAILABLE and (self.ltf_results or self.htf_results):
            ltf_htf_features = self._extract_ltf_htf_features()
            if ltf_htf_features is not None:
                # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –æ—Å–Ω–æ–≤–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
                combined_df = pd.concat([df, ltf_htf_features], axis=1, sort=False)
                df = combined_df
                feature_columns.extend(ltf_htf_features.columns.tolist())
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω—ã LTF/HTF –ø—Ä–∏–∑–Ω–∞–∫–∏: {len(ltf_htf_features.columns)}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        available_features = [col for col in feature_columns if col in df.columns]
        self.features = df[available_features]
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ñ–æ—Ä–º–∞—Ç
        for col in self.features.columns:
            self.features[col] = pd.to_numeric(self.features[col], errors='coerce').fillna(0)
        
        # –°–æ–∑–¥–∞–Ω–∏–µ —Ü–µ–ª–µ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
        self.targets = pd.DataFrame({
            'is_event': df['is_event'] if 'is_event' in df.columns else 0
        })
        
        print(f"‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–∞: {self.features.shape}")
        print(f"   üìä –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π: {len(indicator_fields)}")
        print(f"   üîÑ –õ–∞–≥–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(lag_features)}")
        
        return self.features

    def _extract_ltf_htf_features(self):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ LTF/HTF –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            features_list = []
            
            if self.ltf_results and 'features' in self.ltf_results:
                ltf_features = self.ltf_results['features']
                if isinstance(ltf_features, pd.DataFrame):
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å ltf_ –∫ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫
                    ltf_features_renamed = ltf_features.add_prefix('ltf_')
                    features_list.append(ltf_features_renamed)
            
            if self.htf_results and 'features' in self.htf_results:
                htf_features = self.htf_results['features']
                if isinstance(htf_features, pd.DataFrame):
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å htf_ –∫ –∏–º–µ–Ω–∞–º –∫–æ–ª–æ–Ω–æ–∫
                    htf_features_renamed = htf_features.add_prefix('htf_')
                    features_list.append(htf_features_renamed)
            
            if features_list:
                combined_features = pd.concat(features_list, axis=1, sort=False)
                return combined_features
            
            return None
            
        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è LTF/HTF –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None

    def correlation_analysis(self):
        """–ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print("üîó –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑...")
        
        if self.features is None or self.targets is None:
            return None
        
        correlations = {}
        
        for feature in self.features.columns:
            try:
                feature_data = self.features[feature]
                target_data = self.targets['is_event']
                
                if feature_data.var() > 0:
                    corr = abs(feature_data.corr(target_data))
                    if not np.isnan(corr):
                        correlations[feature] = corr
            except:
                continue
        
        self.correlation_results = dict(sorted(correlations.items(), key=lambda x: x[1], reverse=True))
        
        print(f"‚úÖ –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –∑–∞–≤–µ—Ä—à–µ–Ω ({len(correlations)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
        return self.correlation_results

    def find_optimal_thresholds(self):
        """–ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        print("üéØ –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤...")
        
        if self.features is None or self.targets is None:
            return None
        
        threshold_results = {}
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–æ–ø –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        top_features = list(self.correlation_results.keys())[:50] if self.correlation_results else self.features.columns[:50]
        
        for feature in top_features:
            try:
                feature_data = self.features[feature]
                
                if feature_data.var() == 0:
                    continue
                
                # –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
                feature_range = np.percentile(abs(feature_data), [10, 50, 70, 80, 90, 95])
                best_score = 0
                best_threshold = None
                
                for threshold in feature_range:
                    if threshold > 0:
                        try:
                            binary_feature = (abs(feature_data) > threshold).astype(int)
                            if binary_feature.sum() > 5:  # –ú–∏–Ω–∏–º—É–º –∞–∫—Ç–∏–≤–∞—Ü–∏–π
                                score = roc_auc_score(self.targets['is_event'], binary_feature)
                                if score > best_score:
                                    best_score = score
                                    best_threshold = threshold
                        except:
                            continue
                
                if best_threshold is not None and best_score > 0.55:
                    threshold_results[feature] = {
                        'threshold': best_threshold,
                        'roc_auc': best_score,
                        'activation_rate': (abs(feature_data) > best_threshold).mean()
                    }
            except:
                continue
        
        self.threshold_results = threshold_results
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ä–æ–≥–∏ –¥–ª—è {len(threshold_results)} –ø–æ–ª–µ–π")
        return threshold_results

    def build_scoring_system(self):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã —Å–∫–æ—Ä–∏–Ω–≥–∞ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print("‚öñÔ∏è –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Å–∫–æ—Ä–∏–Ω–≥–∞...")
        
        if self.threshold_results is None:
            return None
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –±–∏–Ω–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        scoring_features = []
        feature_weights = {}
        
        for feature, threshold_info in self.threshold_results.items():
            binary_col = f"{feature}_activated"
            feature_data = self.features[feature]
            self.features[binary_col] = (abs(feature_data) > threshold_info['threshold']).astype(int)
            scoring_features.append(binary_col)
            feature_weights[binary_col] = threshold_info['roc_auc']
        
        if len(scoring_features) == 0:
            return None
        
        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        X = self.features[scoring_features]
        y = self.targets['is_event']
        
        rf = RandomForestClassifier(n_estimators=100, random_state=42)
        rf.fit(X, y)
        
        feature_importance = dict(zip(scoring_features, rf.feature_importances_))
        
        self.scoring_system = {
            'model': rf,
            'features': scoring_features,
            'feature_weights': feature_weights,
            'feature_importance': feature_importance
        }
        
        print(f"‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å–∫–æ—Ä–∏–Ω–≥–∞ —Å–æ–∑–¥–∞–Ω–∞ ({len(scoring_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)")
        return self.scoring_system

    def validate_system(self):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        
        if self.scoring_system is None:
            return None
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
        split_point = int(len(self.features) * 0.7)
        X_train = self.features.iloc[:split_point]
        X_val = self.features.iloc[split_point:]
        y_train = self.targets['is_event'].iloc[:split_point]
        y_val = self.targets['is_event'].iloc[split_point:]
        
        scoring_features = self.scoring_system['features']
        model = self.scoring_system['model']
        
        # –û–±—É—á–µ–Ω–∏–µ –∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        model.fit(X_train[scoring_features], y_train)
        y_pred_proba = model.predict_proba(X_val[scoring_features])[:, 1]
        y_pred = model.predict(X_val[scoring_features])
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        validation_results = {
            'roc_auc': roc_auc_score(y_val, y_pred_proba),
            'accuracy': (y_pred == y_val).mean(),
            'precision': (y_pred * y_val).sum() / max(1, y_pred.sum()),
            'recall': (y_pred * y_val).sum() / max(1, y_val.sum()),
            'event_rate': y_val.mean()
        }
        
        validation_results['lift'] = validation_results['precision'] / max(0.01, validation_results['event_rate'])
        validation_results['meets_requirements'] = (
            validation_results['accuracy'] >= self.config['analysis']['min_accuracy'] and
            validation_results['lift'] >= self.config['analysis']['min_lift']
        )
        
        self.validation_results = validation_results
        
        print(f"   ROC-AUC: {validation_results['roc_auc']:.3f}")
        print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {validation_results['accuracy']:.3f}")
        print(f"   Lift: {validation_results['lift']:.3f}")
        
        return validation_results

    def generate_reports(self):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        print("üìã –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        Path('results').mkdir(exist_ok=True)
        Path('results/reports').mkdir(exist_ok=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self._save_weight_matrix()
        self._save_scoring_config()
        self._create_basic_report()
        
        print("‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Ç—á–µ—Ç—ã —Å–æ–∑–¥–∞–Ω—ã")

    def _save_weight_matrix(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤"""
        if self.scoring_system and self.threshold_results:
            weights_data = []
            
            for feature in self.scoring_system['features']:
                weight = self.scoring_system['feature_importance'].get(feature, 0)
                base_feature = feature.replace('_activated', '')
                threshold_info = self.threshold_results.get(base_feature, {})
                
                weights_data.append({
                    'feature': feature,
                    'weight': weight,
                    'threshold': threshold_info.get('threshold', 0),
                    'roc_auc': threshold_info.get('roc_auc', 0)
                })
            
            weights_df = pd.DataFrame(weights_data)
            weights_df.to_csv('results/weight_matrix.csv', index=False)

    def _save_scoring_config(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∫–æ—Ä–∏–Ω–≥–∞"""
        if self.threshold_results and self.scoring_system:
            config = {
                'thresholds': {k: v['threshold'] for k, v in self.threshold_results.items()},
                'weights': self.scoring_system['feature_importance'],
                'validation_score': self.validation_results['roc_auc'] if self.validation_results else 0
            }
            
            with open('results/scoring_config.json', 'w') as f:
                json.dump(config, f, indent=2)

    def _create_basic_report(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –±–∞–∑–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report_lines = [
            "–§–ò–ù–ê–ù–°–û–í–´–ô –ê–ù–ê–õ–ò–ó–ê–¢–û–† –õ–û–ì–û–í - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –í–ï–†–°–ò–Ø",
            "=" * 60,
            f"–î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "üöÄ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–Ø –ü–†–ò–ú–ï–ù–ï–ù–´:",
            "‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –≤—Å–µ—Ö –ø–æ–ª–µ–π",
            "‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –≤–º–µ—Å—Ç–æ —Ç–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö",
            "‚úÖ LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏ –∞–Ω–∞–ª–∏–∑",
            "‚úÖ –§–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏–π",
            "‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π",
            ""
        ]
        
        if self.parsed_data is not None:
            report_lines.extend([
                f"–ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {len(self.parsed_data)}",
                f"–°–æ–±—ã—Ç–∏—è –Ω–∞–π–¥–µ–Ω—ã: {self.events['total_events'] if self.events else 0}",
                f"–ß–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π: {self.events['event_rate']:.2%}" if self.events else "–ß–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π: 0%"
            ])
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π
        if self.features is not None:
            critical_fields = [col for col in self.features.columns 
                             if any(col.startswith(prefix) for prefix in ['nw', 'ef', 'as', 'vc', 'ze'])]
            
            report_lines.extend([
                "",
                "üéØ –ê–ù–ê–õ–ò–ó –ö–†–ò–¢–ò–ß–ï–°–ö–ò–• –ü–û–õ–ï–ô:",
                f"–ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {len(critical_fields)}",
                f"–ü—Ä–∏–º–µ—Ä—ã –ø–æ–ª–µ–π: {critical_fields[:10] if critical_fields else '–ù–ï –ù–ê–ô–î–ï–ù–´'}",
                ""
            ])
        
        if self.validation_results:
            report_lines.extend([
                "",
                "–†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò:",
                f"ROC-AUC: {self.validation_results['roc_auc']:.3f}",
                f"–¢–æ—á–Ω–æ—Å—Ç—å: {self.validation_results['accuracy']:.3f}",
                f"Lift: {self.validation_results['lift']:.3f}",
                f"–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω—ã: {'–î–ê' if self.validation_results['meets_requirements'] else '–ù–ï–¢'}"
            ])
        
        with open('results/reports/analysis_report.txt', 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

    def run_full_analysis(self, log_file_path, enable_advanced=True):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∑–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π
        """
        print("üöÄ –ó–∞–ø—É—Å–∫ –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ì–û –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        
        try:
            # –≠—Ç–∞–ø 1: –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            print("\nüìä –≠—Ç–∞–ø 1: –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑...")
            self.parse_log_file(log_file_path)
            self.detect_market_events()
            self.analyze_three_phases()
            
            # –≠—Ç–∞–ø 2: –ù–û–í–´–ô LTF/HTF –∞–Ω–∞–ª–∏–∑
            if enable_advanced and ADVANCED_MODULES_AVAILABLE:
                print("\nüîó –≠—Ç–∞–ø 2: LTF/HTF –∞–Ω–∞–ª–∏–∑...")
                self.run_ltf_htf_analysis()
            
            # –≠—Ç–∞–ø 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∞–Ω–∞–ª–∏–∑
            print("\nüèóÔ∏è –≠—Ç–∞–ø 3: –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
            self.build_feature_matrix()
            self.correlation_analysis()
            self.find_optimal_thresholds()
            self.build_scoring_system()
            self.validate_system()
            self.generate_reports()
            
            # –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            print("\nüìÅ –≠—Ç–∞–ø 4: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤...")
            results_folder = self.create_organized_results(log_file_path)
            
            print("üéâ –ò–°–ü–†–ê–í–õ–ï–ù–ù–´–ô –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
            return {
                'status': 'success',
                'results_folder': results_folder,
                'validation_results': self.validation_results,
                'features_count': len(self.features.columns) if self.features is not None else 0,
                'critical_fields_found': self._count_critical_fields(),
                'advanced_modules_used': ADVANCED_MODULES_AVAILABLE
            }
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {
                'status': 'error',
                'message': str(e)
            }

    def _count_critical_fields(self):
        """–ü–æ–¥—Å—á–µ—Ç –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø–æ–ª–µ–π"""
        if self.features is None:
            return 0
        
        critical_prefixes = ['nw', 'ef', 'as', 'vc', 'ze', 'cvz', 'maz']
        critical_fields = [col for col in self.features.columns 
                          if any(col.startswith(prefix) for prefix in critical_prefixes)]
        
        return len(critical_fields)

    def create_organized_results(self, log_file_path):
        """–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)"""
        results_folder = Path('results') / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        results_folder.mkdir(parents=True, exist_ok=True)
        
        return str(results_folder)

    def _describe_phase(self, phase):
        """–û–ø–∏—Å–∞–Ω–∏–µ —Ñ–∞–∑ –¥–ª—è –æ—Ç—á–µ—Ç–æ–≤"""
        descriptions = {
            'preparation': '–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–æ–±—ã—Ç–∏—é - –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–∞–Ω–Ω–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤',
            'culmination': '–ö—É–ª—å–º–∏–Ω–∞—Ü–∏—è - –ø–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –≤—Å–µ—Ö —Å–∏–≥–Ω–∞–ª–æ–≤',
            'development': '–†–∞–∑–≤–∏—Ç–∏–µ - —Å–ø–∞–¥ –∏ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ —Å–æ–±—ã—Ç–∏—è'
        }
        return descriptions.get(phase, '–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–∞—è —Ñ–∞–∑–∞')


# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
def test_enhanced_system(log_file_path):
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã"""
    print("üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–°–ü–†–ê–í–õ–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–´")
    print("=" * 50)
    
    analyzer = FinancialLogAnalyzer()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ä—Å–∏–Ω–≥
    print("1. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞...")
    analyzer.parse_log_file(log_file_path)
    
    if analyzer.parsed_data is not None:
        print(f"   ‚úÖ –ó–∞–ø–∏—Å–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {len(analyzer.parsed_data)}")
        print(f"   üìä –ü–æ–ª–µ–π –Ω–∞–π–¥–µ–Ω–æ: {len(analyzer.parsed_data.columns)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è
        critical_fields = [col for col in analyzer.parsed_data.columns 
                          if any(col.startswith(prefix) for prefix in ['nw', 'ef', 'as', 'vc', 'ze'])]
        
        if critical_fields:
            print(f"   üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ò–ï –ü–û–õ–Ø –ù–ê–ô–î–ï–ù–´: {critical_fields[:5]}")
            print("   ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï –†–ê–ë–û–¢–ê–ï–¢!")
        else:
            print("   ‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–æ–ª—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    return analyzer


if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã
    import sys
    
    if len(sys.argv) > 1:
        log_file = sys.argv[1]
        test_enhanced_system(log_file)
    else:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python main.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É_–ª–æ–≥–∞>")