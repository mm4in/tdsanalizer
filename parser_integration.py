#!/usr/bin/env python3
"""
–ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ô –ú–û–î–£–õ–¨ - DATA-DRIVEN –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø 
–ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω—ã –Ω–∞—Ä—É—à–µ–Ω–∏—è data-driven –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤

–ö–õ–Æ–ß–ï–í–´–ï –ü–†–ò–ù–¶–ò–ü–´:
‚úÖ –ù–ò –û–î–ù–û –ø–æ–ª–µ –Ω–µ –∏–º–µ–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
‚úÖ –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞–≤–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π
‚úÖ –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¢–û–õ–¨–ö–û: –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã > –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ  
‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (ROC-AUC)
‚úÖ –ü–æ–ª–Ω–∞—è data-driven –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
"""

import pandas as pd
import numpy as np
import re
from pathlib import Path
from typing import Dict, Any, Tuple, Set
from advanced_log_parser import AdvancedLogParser

class ParserIntegration:
    """
    –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: data-driven –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –ë–ï–ó –Ω–∞—Ä—É—à–µ–Ω–∏–π
    """
    
    def __init__(self, main_analyzer=None):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å data-driven –ø–æ–¥—Ö–æ–¥–æ–º"""
        self.advanced_parser = AdvancedLogParser()
        self.main_analyzer = main_analyzer
        self.parsing_results = {}
        self.special_values_detected = {}
        
        # –ì–†–£–ü–ü–´ –ò–ù–î–ò–ö–ê–¢–û–†–ù–´–• –ü–û–õ–ï–ô (–í–°–ï –†–ê–í–ù–´!)
        self.indicator_groups = {
            'group_1': ['rd', 'md', 'cd', 'cmd', 'macd', 'od', 'dd', 'cvd', 'drd', 'ad', 'ed', 'hd', 'sd'],
            'group_2': ['ro', 'mo', 'co', 'cz', 'do', 'ae', 'so'],
            'group_3': ['rz', 'mz', 'ciz', 'sz', 'dz', 'cvz', 'maz', 'oz'],
            'group_4': ['ef', 'wv', 'vc', 'ze', 'nw', 'as', 'vw'],
            'group_5': ['bs', 'wa', 'pd']
        }
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Å–ø—Ä–∞–≤–æ—á–Ω—ã–µ)
        self.metadata_fields = ['open', 'high', 'low', 'close', 'volume', 'range']
        
        # DATA-DRIVEN –ü–†–ò–û–†–ò–¢–ò–ó–ê–¶–ò–Ø: —Ç–æ–ª—å–∫–æ 2 —É—Ä–æ–≤–Ω—è
        self.priority_levels = {
            'indicators': 1.0,    # –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞–≤–Ω—ã
            'metadata': 0.1       # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Å–ø—Ä–∞–≤–æ—á–Ω–æ
        }
        
    def replace_old_parser(self, log_file_path: str) -> Dict[str, pd.DataFrame]:
        """–ó–∞–º–µ–Ω–∞ –ø–∞—Ä—Å–µ—Ä–∞ —Å data-driven –ø–æ–¥—Ö–æ–¥–æ–º"""
        print("üîÑ DATA-DRIVEN –ø–∞—Ä—Å–µ—Ä (–ë–ï–ó –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π)")
        
        # –ü–∞—Ä—Å–∏–Ω–≥
        full_data = self.advanced_parser.parse_log_file(log_file_path)
        
        if full_data.empty:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –¥–∞–Ω–Ω—ã–µ")
            return {}
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        self._detect_special_patterns(full_data)
        
        # LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ
        ltf_data, htf_data = self.advanced_parser.get_ltf_htf_separation(full_data)
        
        results = {
            'full_data': full_data,
            'ltf_data': ltf_data,
            'htf_data': htf_data,
            'data_driven_analysis': self._analyze_field_activity(full_data)
        }
        
        self.parsing_results = results
        
        print("‚úÖ Data-driven –ø–∞—Ä—Å–µ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω")
        return results
    
    def _detect_special_patterns(self, data: pd.DataFrame) -> None:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        print("üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤...")
        
        special_patterns = set()
        
        for col in data.columns:
            if col not in self.metadata_fields:
                unique_values = data[col].dropna().astype(str).unique()
                
                for value in unique_values:
                    if re.match(r'^!+$', str(value)):
                        special_patterns.add(str(value))
        
        if special_patterns:
            self.special_values_detected = {}
            base_value = 1000
            
            for pattern in sorted(special_patterns, key=len):
                self.special_values_detected[pattern] = base_value
                base_value += 500
            
            print(f"   üìä –ù–∞–π–¥–µ–Ω–æ: {list(special_patterns)}")
        else:
            print("   ‚ÑπÔ∏è –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
    
    def _analyze_field_activity(self, data: pd.DataFrame) -> Dict[str, Any]:
        """DATA-DRIVEN –∞–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª–µ–π"""
        
        activity_analysis = {}
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –ë–ï–ó –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –≤–µ—Å–æ–≤
        for group_name, prefixes in self.indicator_groups.items():
            group_fields = []
            for prefix in prefixes:
                group_fields.extend([col for col in data.columns 
                                   if col.startswith(prefix) and col not in self.metadata_fields])
            
            if group_fields:
                total_activations = 0
                non_zero_fields = 0
                
                for field in group_fields:
                    field_data = data[field].dropna()
                    if len(field_data) > 0:
                        activations = ((field_data.astype(str) != '0') & 
                                     (field_data.astype(str) != 'nan') & 
                                     (field_data.astype(str) != '')).sum()
                        
                        if activations > 0:
                            non_zero_fields += 1
                            total_activations += activations
                
                activity_analysis[group_name] = {
                    'fields_count': len(group_fields),
                    'active_fields': non_zero_fields,
                    'total_activations': total_activations,
                    'activity_rate': total_activations / max(1, len(data)),
                    'data_driven_priority': 1.0  # –í–°–ï –ì–†–£–ü–ü–´ –†–ê–í–ù–´!
                }
        
        # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        metadata_activations = 0
        for field in self.metadata_fields:
            if field in data.columns:
                metadata_activations += data[field].notna().sum()
        
        activity_analysis['metadata'] = {
            'fields_count': len([f for f in self.metadata_fields if f in data.columns]),
            'total_activations': metadata_activations,
            'activity_rate': metadata_activations / max(1, len(data)),
            'data_driven_priority': 0.1  # –¢–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–∏–∂–µ
        }
        
        return activity_analysis
    
    def get_features_for_main_system(self) -> pd.DataFrame:
        """
        DATA-DRIVEN —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ë–ï–ó –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π
        """
        if 'full_data' not in self.parsing_results:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ replace_old_parser()")
            return pd.DataFrame()
        
        print("üéØ DATA-DRIVEN —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (–í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞–≤–Ω—ã)")
        
        full_data = self.parsing_results['full_data']
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å data-driven –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π
        features_df = self._create_data_driven_features(full_data)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(features_df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        
        # –û—Ç—á–µ—Ç –æ data-driven –ø–æ–¥—Ö–æ–¥–µ
        self._report_data_driven_approach(features_df)
        
        return features_df
    
    def _create_data_driven_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        DATA-DRIVEN —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ä–∞–≤–Ω—ã
        """
        features = pd.DataFrame(index=data.index)
        
        print("üéØ –®–ê–ì 1: –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è (–í–°–ï –†–ê–í–ù–´)")
        self._add_equal_indicator_features(features, data)
        
        print("üîÑ –®–ê–ì 2: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –æ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤") 
        self._add_indicator_derivatives(features, data)
        
        print("üìä –®–ê–ì 3: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Å–ø—Ä–∞–≤–æ—á–Ω–æ)")
        self._add_metadata_features(features, data)
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        features.fillna(0, inplace=True)
        
        return features
    
    def _add_equal_indicator_features(self, features: pd.DataFrame, data: pd.DataFrame) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –†–ê–í–ù–´–ú —Å—Ç–∞—Ç—É—Å–æ–º"""
        
        total_indicator_fields = 0
        
        # –í–°–ï –≥—Ä—É–ø–ø—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –û–î–ò–ù–ê–ö–û–í–û
        for group_name, prefixes in self.indicator_groups.items():
            group_fields = []
            for prefix in prefixes:
                group_fields.extend([col for col in data.columns 
                                   if col.startswith(prefix) and col not in self.metadata_fields])
            
            print(f"   {group_name}: {len(group_fields)} –ø–æ–ª–µ–π (—Ä–∞–≤–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)")
            
            for field in group_fields:
                # 1. –û–°–ù–û–í–ù–û–ï –ó–ù–ê–ß–ï–ù–ò–ï (–≤—Å–µ –ø–æ–ª—è —Ä–∞–≤–Ω—ã)
                numeric_data = self._safe_numeric_conversion(data[field])
                features[f"IND_{field}"] = numeric_data  # –ü—Ä–µ—Ñ–∏–∫—Å IND_ = –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
                
                # 2. –ê–ö–¢–ò–í–ù–û–°–¢–¨ –ü–û–õ–Ø
                active_mask = ((data[field].astype(str) != '0') & 
                              (data[field].astype(str) != 'nan') & 
                              (data[field].astype(str) != '') & 
                              data[field].notna())
                features[f"IND_{field}_ACTIVE"] = active_mask.astype(int)
                
                # 3. –°–ü–ï–¶–ò–ê–õ–¨–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø (–µ—Å–ª–∏ –µ—Å—Ç—å)
                if self.special_values_detected:
                    for special_val, numeric_equiv in self.special_values_detected.items():
                        special_mask = (data[field].astype(str) == special_val)
                        if special_mask.sum() > 0:
                            clean_name = special_val.replace('!', 'EXCL')
                            features[f"IND_{field}_{clean_name}"] = (special_mask.astype(int) * numeric_equiv)
                
                total_indicator_fields += 1
        
        print(f"   ‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {total_indicator_fields} –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π —Å –†–ê–í–ù–´–ú —Å—Ç–∞—Ç—É—Å–æ–º")
    
    def _add_indicator_derivatives(self, features: pd.DataFrame, data: pd.DataFrame) -> None:
        """–ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¢–û–õ–¨–ö–û –æ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π"""
        
        # –í—ã–±–∏—Ä–∞–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è –¥–ª—è –ª–∞–≥–æ–≤
        indicator_cols = [col for col in features.columns if col.startswith('IND_')]
        
        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 10 –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        for col in indicator_cols[:10]:
            if col in features.columns:
                # –õ–∞–≥–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
                features[f"{col}_LAG1"] = features[col].shift(1)
                features[f"{col}_LAG2"] = features[col].shift(2)
                
                # –ò–∑–º–µ–Ω–µ–Ω–∏—è
                features[f"{col}_DIFF"] = features[col].diff()
                
                # –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ä–µ–¥–Ω–∏–µ
                features[f"{col}_MA3"] = features[col].rolling(3).mean()
    
    def _add_metadata_features(self, features: pd.DataFrame, data: pd.DataFrame) -> None:
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö —Å –Ω–∏–∑–∫–∏–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º"""
        
        metadata_weight = self.priority_levels['metadata']  # 0.1
        metadata_count = 0
        
        for field in self.metadata_fields:
            if field in data.columns:
                features[f"META_{field}"] = data[field] * metadata_weight
                metadata_count += 1
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –æ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        required_ohlc = ['open', 'high', 'low', 'close']
        if all(f"META_{col}" in features.columns for col in required_ohlc):
            features['META_price_range'] = ((features['META_high'] - 
                                           features['META_low']) * metadata_weight)
        
        print(f"   üìä –î–æ–±–∞–≤–ª–µ–Ω–æ {metadata_count} –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)")
    
    def _safe_numeric_conversion(self, series: pd.Series) -> pd.Series:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è"""
        result = series.copy()
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        for special_val, numeric_equiv in self.special_values_detected.items():
            mask = (result.astype(str) == special_val)
            result.loc[mask] = numeric_equiv
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —á–∏—Å–ª–∞
        result = pd.to_numeric(result, errors='coerce').fillna(0)
        
        return result
    
    def _report_data_driven_approach(self, features: pd.DataFrame) -> None:
        """–û—Ç—á–µ—Ç –æ data-driven –ø–æ–¥—Ö–æ–¥–µ"""
        
        print("\nüìä DATA-DRIVEN –û–¢–ß–ï–¢:")
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–æ —Ç–∏–ø–∞–º
        indicator_cols = [col for col in features.columns if col.startswith('IND_')]
        meta_cols = [col for col in features.columns if col.startswith('META_')]
        derivative_cols = [col for col in features.columns if any(x in col for x in ['_LAG', '_DIFF', '_MA'])]
        
        print(f"   üéØ –ò–ù–î–ò–ö–ê–¢–û–†–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(indicator_cols)} (–†–ê–í–ù–´–ô –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)")
        print(f"   üîÑ –ü–†–û–ò–ó–í–û–î–ù–´–• –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(derivative_cols)}")
        print(f"   üìä –ú–ï–¢–ê–î–ê–ù–ù–´–•: {len(meta_cols)} (—Å–ø—Ä–∞–≤–æ—á–Ω–æ x0.1)")
        
        # –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
        total_indicator_features = len(indicator_cols) + len(derivative_cols)
        total_metadata_features = len(meta_cols)
        
        if total_metadata_features > 0:
            ratio = total_indicator_features / total_metadata_features
            print(f"   üìà –°–û–û–¢–ù–û–®–ï–ù–ò–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã/–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {ratio:.1f}:1")
        
        print("   ‚úÖ DATA-DRIVEN: –í–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—Å—è —á–µ—Ä–µ–∑ ROC-AUC!")
    
    def get_targets_for_main_system(self) -> pd.DataFrame:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ data-driven –∞–Ω–∞–ª–∏–∑–∞"""
        if 'full_data' not in self.parsing_results:
            return pd.DataFrame()
        
        print("üéØ Data-driven –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π...")
        
        full_data = self.parsing_results['full_data']
        targets = pd.DataFrame(index=full_data.index)
        targets['is_event'] = 0
        
        # –°–æ–±—ã—Ç–∏—è –ø–æ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º (–µ—Å–ª–∏ –µ—Å—Ç—å)
        if self.special_values_detected:
            special_events = pd.Series(0, index=full_data.index)
            
            for col in full_data.columns:
                if col not in self.metadata_fields:
                    for special_val in self.special_values_detected.keys():
                        mask = (full_data[col].astype(str) == special_val)
                        special_events += mask.astype(int)
            
            targets['is_event'] = (special_events > 0).astype(int)
            targets['special_activations'] = special_events
        
        # –°–æ–±—ã—Ç–∏—è –ø–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º —ç–∫—Å—Ç—Ä–µ–º—É–º–∞–º (data-driven –ø–æ—Ä–æ–≥–∏)
        extreme_events = pd.Series(0, index=full_data.index)
        
        for group_name, prefixes in self.indicator_groups.items():
            for prefix in prefixes:
                group_fields = [col for col in full_data.columns 
                              if col.startswith(prefix) and col not in self.metadata_fields]
                
                for field in group_fields:
                    numeric_data = self._safe_numeric_conversion(full_data[field])
                    if len(numeric_data.dropna()) > 10:
                        # Data-driven –ø–æ—Ä–æ–≥–∏ (–±–µ–∑ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏)
                        q95 = numeric_data.quantile(0.95)
                        q05 = numeric_data.quantile(0.05)
                        
                        extreme_mask = (numeric_data > q95) | (numeric_data < q05)
                        extreme_events += extreme_mask.astype(int)
        
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
        targets['is_event'] = ((targets['is_event'] == 1) | (extreme_events > 2)).astype(int)
        targets['extreme_score'] = extreme_events
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {targets['is_event'].sum()}")
        
        return targets
    
    def integration_report(self) -> str:
        """–û—Ç—á–µ—Ç –æ–± data-driven –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        if not self.parsing_results:
            return "‚ùå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞"
        
        analysis = self.parsing_results.get('data_driven_analysis', {})
        
        report = [
            "üéØ DATA-DRIVEN –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø (–ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò)",
            "=" * 55,
            "‚úÖ –ü–†–ò–ù–¶–ò–ü: –ù–ò –û–î–ù–û –ø–æ–ª–µ –Ω–µ –∏–º–µ–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞",
            "‚úÖ –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç—Å—è –æ–¥–∏–Ω–∞–∫–æ–≤–æ",
            "‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É",
            ""
        ]
        
        # –ê–Ω–∞–ª–∏–∑ –ø–æ –≥—Ä—É–ø–ø–∞–º (–≤—Å–µ —Ä–∞–≤–Ω—ã)
        report.append("üìä –ê–ö–¢–ò–í–ù–û–°–¢–¨ –ì–†–£–ü–ü (–í–°–ï –†–ê–í–ù–´):")
        
        for group in ['group_1', 'group_2', 'group_3', 'group_4', 'group_5']:
            if group in analysis:
                stats = analysis[group]
                activity = stats['total_activations']
                
                if activity > 100:
                    status = "üî• –í–´–°–û–ö–ê–Ø"
                elif activity > 50:
                    status = "‚ö° –°–†–ï–î–ù–Ø–Ø"
                else:
                    status = "üìä –ù–ò–ó–ö–ê–Ø"
                
                report.append(f"   {status} {group.upper()}: "
                            f"{stats['fields_count']} –ø–æ–ª–µ–π, "
                            f"{activity} –∞–∫—Ç–∏–≤–∞—Ü–∏–π (—Ä–∞–≤–Ω—ã–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)")
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        if 'metadata' in analysis:
            meta_stats = analysis['metadata']
            report.extend([
                "",
                f"üìä –ú–ï–¢–ê–î–ê–ù–ù–´–ï (—Å–ø—Ä–∞–≤–æ—á–Ω–æ): "
                f"{meta_stats['fields_count']} –ø–æ–ª–µ–π, "
                f"{meta_stats['total_activations']} –∞–∫—Ç–∏–≤–∞—Ü–∏–π "
                f"(–Ω–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç x0.1)"
            ])
        
        report.extend([
            "",
            "‚úÖ DATA-DRIVEN –ü–†–ò–ù–¶–ò–ü–´ –°–û–ë–õ–Æ–î–ï–ù–´:",
            "   üéØ –í—Å–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –ø–æ–ª—É—á–∏–ª–∏ —Ä–∞–≤–Ω—ã–π —Å—Ç–∞—Ç—É—Å",
            "   üìä –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ - —Ç–æ–ª—å–∫–æ —Å–ø—Ä–∞–≤–æ—á–Ω–æ",
            "   üîç –í–∞–∂–Ω–æ—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ ROC-AUC –≤ main.py",
            "   ‚öñÔ∏è –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
            "",
            "üéä –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ë–ï–ó –ù–ê–†–£–®–ï–ù–ò–Ø –ü–†–ò–ù–¶–ò–ü–û–í –¢–ó!"
        ])
        
        return "\n".join(report)


if __name__ == "__main__":
    # –¢–µ—Å—Ç data-driven –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏
    test_log_path = "data/dslog_btc_0508240229_ltf.txt"
    
    if Path(test_log_path).exists():
        integration = ParserIntegration()
        results = integration.replace_old_parser(test_log_path)
        
        if results:
            features = integration.get_features_for_main_system()
            print(integration.integration_report())
            print("\nüéâ DATA-DRIVEN –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø –ë–ï–ó –ù–ê–†–£–®–ï–ù–ò–ô –ì–û–¢–û–í–ê!")
    else:
        print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {test_log_path}")
