#!/usr/bin/env python3
"""
–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞, –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –ª–æ–≥–æ–≤
"""

import pandas as pd
import numpy as np
import re
import json
from pathlib import Path
from datetime import datetime, timedelta
import argparse

class DataProcessor:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.supported_formats = ['.txt', '.log', '.csv']
        self.validation_stats = {}
    
    def validate_log_format(self, file_path):
        """–í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞ –ª–æ–≥ —Ñ–∞–π–ª–∞"""
        print(f"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–∞–π–ª–∞: {file_path}")
        
        validation_results = {
            'file_exists': False,
            'readable': False,
            'correct_format': False,
            'line_count': 0,
            'valid_lines': 0,
            'timestamp_format': False,
            'required_fields': False,
            'errors': []
        }
        
        file_path = Path(file_path)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞
        if not file_path.exists():
            validation_results['errors'].append("–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
            return validation_results
        
        validation_results['file_exists'] = True
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            validation_results['readable'] = True
            validation_results['line_count'] = len(lines)
            
        except Exception as e:
            validation_results['errors'].append(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")
            return validation_results
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–æ–∫
        valid_line_count = 0
        timestamp_valid = False
        required_fields_found = False
        
        for i, line in enumerate(lines[:100]):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–≤—ã–µ 100 —Å—Ç—Ä–æ–∫
            line = line.strip()
            if not line:
                continue
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ timestamp
            timestamp_match = re.match(r'\[([^\]]+)\]:', line)
            if timestamp_match:
                try:
                    pd.to_datetime(timestamp_match.group(1))
                    timestamp_valid = True
                except:
                    pass
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞
            if '|' in line and 'LTF|' in line:
                parts = line.split('|')
                if len(parts) >= 6:
                    valid_line_count += 1
                    
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π
                    if 'o:' in line and 'h:' in line and 'l:' in line and 'c:' in line:
                        required_fields_found = True
        
        validation_results['valid_lines'] = valid_line_count
        validation_results['timestamp_format'] = timestamp_valid
        validation_results['required_fields'] = required_fields_found
        validation_results['correct_format'] = (
            valid_line_count > 0 and timestamp_valid and required_fields_found
        )
        
        if validation_results['correct_format']:
            print("‚úÖ –§–∞–π–ª –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
        else:
            print("‚ùå –§–∞–π–ª –Ω–µ –ø—Ä–æ—à–µ–ª –≤–∞–ª–∏–¥–∞—Ü–∏—é")
            for error in validation_results['errors']:
                print(f"   - {error}")
        
        return validation_results
    
    def clean_log_data(self, input_file, output_file=None):
        """–û—á–∏—Å—Ç–∫–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –ª–æ–≥ –¥–∞–Ω–Ω—ã—Ö"""
        print(f"üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {input_file}")
        
        if output_file is None:
            output_file = str(Path(input_file).with_suffix('.cleaned.txt'))
        
        cleaned_lines = []
        stats = {
            'total_lines': 0,
            'valid_lines': 0,
            'cleaned_lines': 0,
            'removed_duplicates': 0,
            'fixed_timestamps': 0
        }
        
        seen_lines = set()
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                stats['total_lines'] += 1
                line = line.strip()
                
                if not line:
                    continue
                
                # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
                if line in seen_lines:
                    stats['removed_duplicates'] += 1
                    continue
                seen_lines.add(line)
                
                # –ë–∞–∑–æ–≤–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∞
                if not (line.startswith('[') and 'LTF|' in line):
                    continue
                
                stats['valid_lines'] += 1
                
                # –û—á–∏—Å—Ç–∫–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
                cleaned_line = self._clean_line(line)
                if cleaned_line:
                    cleaned_lines.append(cleaned_line)
                    stats['cleaned_lines'] += 1
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(cleaned_lines))
        
        print(f"‚úÖ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_file}")
        print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Ç—Ä–æ–∫: {stats['total_lines']}")
        print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {stats['valid_lines']}")
        print(f"   –û—á–∏—â–µ–Ω–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {stats['cleaned_lines']}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {stats['removed_duplicates']}")
        
        return output_file, stats
    
    def _clean_line(self, line):
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        try:
            # –£–¥–∞–ª–µ–Ω–∏–µ –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
            line = re.sub(r'\s+', ' ', line)
            
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π
            line = line.replace(' | ', '|').replace('| ', '|').replace(' |', '|')
            
            # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ common –ø—Ä–æ–±–ª–µ–º —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            line = re.sub(r'([a-zA-Z]+)(\d+)--?([+-]?\d+\.?\d*)', r'\1\2-\3', line)
            
            return line
            
        except Exception:
            return None
    
    def convert_google_sheets_export(self, input_file, output_file=None):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —ç–∫—Å–ø–æ—Ä—Ç–∞ –∏–∑ Google Sheets"""
        print(f"üìä –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏–∑ Google Sheets: {input_file}")
        
        if output_file is None:
            output_file = str(Path(input_file).with_suffix('.converted.txt'))
        
        # –ü–æ–ø—ã—Ç–∫–∞ —á—Ç–µ–Ω–∏—è –∫–∞–∫ CSV
        try:
            df = pd.read_csv(input_file)
            
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –≤ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            converted_lines = []
            
            for _, row in df.iterrows():
                # –ü–æ–ø—ã—Ç–∫–∞ —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ —Ñ–æ—Ä–º–∞—Ç–∞ –ª–æ–≥–∞
                # –≠—Ç–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
                line_parts = [str(val) for val in row.values if pd.notna(val)]
                if len(line_parts) > 5:
                    # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∞–ª–∏–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
                    timestamp = pd.Timestamp.now().strftime('[%Y-%m-%dT%H:%M:%S.000+03:00]')
                    converted_line = f"{timestamp}: {' | '.join(line_parts)}"
                    converted_lines.append(converted_line)
            
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(converted_lines))
            
            print(f"‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {output_file}")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏: {e}")
            return None
        
        return output_file
    
    def split_by_events(self, input_file, output_dir=None):
        """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ª–æ–≥–∞ –ø–æ —Å–æ–±—ã—Ç–∏—è–º"""
        print(f"‚úÇÔ∏è –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å–æ–±—ã—Ç–∏—è–º: {input_file}")
        
        if output_dir is None:
            output_dir = Path(input_file).parent / 'split_events'
        
        output_dir = Path(output_dir)
        output_dir.mkdir(exist_ok=True)
        
        current_event = None
        current_lines = []
        event_files = []
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å–æ–±—ã—Ç–∏—è
                match = re.search(r'LTF\|([^|]+)\|', line)
                if match:
                    event_name = match.group(1)
                    
                    # –ï—Å–ª–∏ –Ω–∞—á–∞–ª–æ—Å—å –Ω–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ
                    if current_event != event_name:
                        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Å–æ–±—ã—Ç–∏—è
                        if current_event and current_lines:
                            event_file = output_dir / f"{current_event}.txt"
                            with open(event_file, 'w', encoding='utf-8') as ef:
                                ef.write('\n'.join(current_lines))
                            event_files.append(event_file)
                        
                        # –ù–∞—á–∞–ª–æ –Ω–æ–≤–æ–≥–æ —Å–æ–±—ã—Ç–∏—è
                        current_event = event_name
                        current_lines = []
                
                current_lines.append(line)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–±—ã—Ç–∏—è
        if current_event and current_lines:
            event_file = output_dir / f"{current_event}.txt"
            with open(event_file, 'w', encoding='utf-8') as ef:
                ef.write('\n'.join(current_lines))
            event_files.append(event_file)
        
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(event_files)} —Ñ–∞–π–ª–æ–≤ —Å–æ–±—ã—Ç–∏–π –≤ {output_dir}")
        return event_files
    
    def merge_log_files(self, input_files, output_file):
        """–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–æ–≥ —Ñ–∞–π–ª–æ–≤"""
        print(f"üîó –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ {len(input_files)} —Ñ–∞–π–ª–æ–≤")
        
        all_lines = []
        
        for file_path in input_files:
            print(f"   –û–±—Ä–∞–±–æ—Ç–∫–∞: {file_path}")
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                all_lines.extend([line.strip() for line in lines if line.strip()])
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ timestamp –µ—Å–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ
        try:
            def extract_timestamp(line):
                match = re.match(r'\[([^\]]+)\]:', line)
                if match:
                    return pd.to_datetime(match.group(1))
                return pd.Timestamp.min
            
            all_lines.sort(key=extract_timestamp)
            print("‚úÖ –°—Ç—Ä–æ–∫–∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏")
            
        except Exception as e:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏: {e}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        unique_lines = list(dict.fromkeys(all_lines))
        removed_duplicates = len(all_lines) - len(unique_lines)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(unique_lines))
        
        print(f"‚úÖ –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {output_file}")
        print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {len(all_lines)}")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {len(unique_lines)}")
        print(f"   –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {removed_duplicates}")
        
        return output_file
    
    def generate_sample_data(self, output_file, num_records=200):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–º–µ—Ä–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
        print(f"üé≤ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è {num_records} —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–ø–∏—Å–µ–π")
        
        sample_lines = []
        base_time = pd.Timestamp('2024-08-05 09:00:00+03:00')
        
        # –ì—Ä—É–ø–ø—ã –ø–æ–ª–µ–π –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        field_groups = {
            'group_1': ['rd', 'md', 'cd', 'cmd', 'macd'],
            'group_2': ['ro', 'mo', 'co', 'cz', 'do'],
            'group_3': ['rz', 'mz', 'ciz', 'sz', 'cvz'],
            'group_4': ['ef', 'wv', 'vc', 'ze', 'as'],
            'group_5': ['bs', 'wa', 'pd']
        }
        
        timeframes = [2, 5, 15, 30]
        
        for i in range(num_records):
            timestamp = base_time + pd.Timedelta(minutes=i)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è OHLC –¥–∞–Ω–Ω—ã—Ö
            base_price = 50000 + np.random.normal(0, 1000) * (1 + i/1000)  # –î–æ–±–∞–≤–ª—è–µ–º —Ç—Ä–µ–Ω–¥
            volatility = np.random.uniform(0.5, 3.0)
            
            open_price = base_price + np.random.normal(0, 50)
            high_price = open_price + abs(np.random.exponential(50 * volatility))
            low_price = open_price - abs(np.random.exponential(50 * volatility))
            close_price = open_price + np.random.normal(0, 100 * volatility)
            
            # –°–≤–æ–π—Å—Ç–≤–∞ —Å–≤–µ—á–∏
            color = "GREEN" if close_price > open_price else "RED"
            change = ((close_price - open_price) / open_price) * 100
            volume = f"{np.random.uniform(0.5, 15):.1f}K"
            
            candle_types = ["NORMAL", "BIG_BODY", "DOJI", "PIN_TOP", "PIN_BOTTOM"]
            candle_type = np.random.choice(candle_types, p=[0.6, 0.2, 0.1, 0.05, 0.05])
            
            completion = np.random.randint(10, 100)
            movement_24h = np.random.uniform(-25, 25)
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ–ª–µ–π
            fields = []
            
            # –°–æ–±—ã—Ç–∏–π–Ω–∞—è –ª–æ–≥–∏–∫–∞ - –±–æ–ª—å—à–µ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ "—Å–æ–±—ã—Ç–∏—è–º–∏"
            is_event_period = (i % 50) < 5  # –ö–∞–∂–¥—ã–µ 50 –∑–∞–ø–∏—Å–µ–π - 5 –∑–∞–ø–∏—Å–µ–π —Å–æ–±—ã—Ç–∏—è
            activation_probability = 0.4 if is_event_period else 0.1
            
            for group_name, group_fields in field_groups.items():
                for field_base in group_fields:
                    if np.random.random() < activation_probability:
                        tf = np.random.choice(timeframes)
                        
                        if group_name == 'group_3':  # Z-scores
                            value = np.random.uniform(-4.0, 4.0)
                            fields.append(f"{field_base}{tf}--{value:.2f}")
                        elif group_name == 'group_1':  # Percentages
                            value = np.random.uniform(-50, 50)
                            fields.append(f"{field_base}{tf}-{value:.1f}%")
                        else:  # Regular values
                            value = np.random.randint(10, 100)
                            fields.append(f"{field_base}{tf}-{value}")
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è
            progress_fields = []
            for tf in [2, 5, 15, 30]:
                if np.random.random() > 0.5:
                    progress = np.random.randint(0, 100)
                    progress_fields.append(f"p{tf}-{progress}")
            
            # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –ª–æ–≥–∞
            log_line = (
                f"[{timestamp.strftime('%Y-%m-%dT%H:%M:%S.000+03:00')}]: "
                f"LTF|event_sample_{i // 50 + 1}|1|{timestamp.strftime('%Y-%m-%d %H:%M')}|"
                f"{color}|{change:.2f}%|{volume}|{candle_type}|{completion}%|{movement_24h:.2f}%_24h|"
                f"o:{open_price:.1f}|h:{high_price:.1f}|l:{low_price:.1f}|c:{close_price:.1f}|"
                f"rng:{high_price-low_price:.1f}"
            )
            
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª–µ–π
            all_fields = fields + progress_fields
            if all_fields:
                log_line += "|" + ",".join(all_fields)
            
            sample_lines.append(log_line)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(sample_lines))
        
        print(f"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã: {output_file}")
        print(f"   –ó–∞–ø–∏—Å–µ–π: {num_records}")
        print(f"   –°–æ–±—ã—Ç–∏—è: {num_records // 50}")
        print(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω: {sample_lines[0].split(']')[0][1:]} - {sample_lines[-1].split(']')[0][1:]}")
        
        return output_file
    
    def analyze_data_quality(self, file_path):
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        print(f"üìä –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: {file_path}")
        
        quality_report = {
            'file_info': {},
            'structure_analysis': {},
            'field_analysis': {},
            'temporal_analysis': {},
            'recommendations': []
        }
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ñ–∞–π–ª–µ
        file_path = Path(file_path)
        quality_report['file_info'] = {
            'file_size': file_path.stat().st_size,
            'created': datetime.fromtimestamp(file_path.stat().st_ctime).isoformat(),
            'modified': datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
        }
        
        # –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        total_lines = len(lines)
        valid_lines = 0
        timestamps = []
        field_counts = {}
        all_fields = set()
        
        for line in lines:
            line = line.strip()
            if not line or not line.startswith('['):
                continue
            
            valid_lines += 1
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ timestamp
            timestamp_match = re.match(r'\[([^\]]+)\]:', line)
            if timestamp_match:
                try:
                    ts = pd.to_datetime(timestamp_match.group(1))
                    timestamps.append(ts)
                except:
                    pass
            
            # –ü–æ–¥—Å—á–µ—Ç –ø–æ–ª–µ–π
            field_matches = re.findall(r'([a-zA-Z]+\d*)-?([^,|]+)', line)
            line_fields = len(field_matches)
            field_counts[line_fields] = field_counts.get(line_fields, 0) + 1
            
            for field_name, _ in field_matches:
                all_fields.add(field_name)
        
        quality_report['structure_analysis'] = {
            'total_lines': total_lines,
            'valid_lines': valid_lines,
            'valid_ratio': valid_lines / total_lines if total_lines > 0 else 0,
            'unique_fields': len(all_fields),
            'avg_fields_per_line': np.mean(list(field_counts.keys())) if field_counts else 0,
            'field_distribution': field_counts
        }
        
        # –í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑
        if timestamps:
            timestamps = sorted(timestamps)
            time_diffs = [
                (timestamps[i+1] - timestamps[i]).total_seconds() 
                for i in range(len(timestamps)-1)
            ]
            
            quality_report['temporal_analysis'] = {
                'time_span': (timestamps[-1] - timestamps[0]).total_seconds() / 3600,  # hours
                'avg_interval': np.mean(time_diffs) if time_diffs else 0,  # seconds
                'irregular_intervals': sum(1 for diff in time_diffs if diff > 120),  # > 2 minutes
                'data_gaps': sum(1 for diff in time_diffs if diff > 300)  # > 5 minutes
            }
        
        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        recommendations = []
        
        if quality_report['structure_analysis']['valid_ratio'] < 0.8:
            recommendations.append("–ù–∏–∑–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
        
        if quality_report['temporal_analysis'].get('data_gaps', 0) > 5:
            recommendations.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–ø—É—Å–∫–∏ –≤–æ –≤—Ä–µ–º–µ–Ω–∏")
        
        if quality_report['structure_analysis']['unique_fields'] < 10:
            recommendations.append("–ú–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π - –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º")
        
        if not recommendations:
            recommendations.append("–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö —Ö–æ—Ä–æ—à–µ–µ")
        
        quality_report['recommendations'] = recommendations
        
        # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
        print("\nüìã –û–¢–ß–ï–¢ –û –ö–ê–ß–ï–°–¢–í–ï –î–ê–ù–ù–´–•:")
        print(f"   –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {quality_report['file_info']['file_size'] / 1024:.1f} KB")
        print(f"   –í–∞–ª–∏–¥–Ω—ã—Ö —Å—Ç—Ä–æ–∫: {valid_lines}/{total_lines} ({quality_report['structure_analysis']['valid_ratio']:.1%})")
        print(f"   –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π: {quality_report['structure_analysis']['unique_fields']}")
        print(f"   –í—Ä–µ–º–µ–Ω–Ω–æ–π –æ—Ö–≤–∞—Ç: {quality_report['temporal_analysis'].get('time_span', 0):.1f} —á–∞—Å–æ–≤")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        for rec in recommendations:
            print(f"   - {rec}")
        
        return quality_report


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —É—Ç–∏–ª–∏—Ç –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–£—Ç–∏–ª–∏—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('command', choices=[
        'validate', 'clean', 'convert', 'split', 'merge', 'sample', 'quality'
    ], help='–ö–æ–º–∞–Ω–¥–∞ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è')
    parser.add_argument('input', help='–í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –∏–ª–∏ –ø–∞–ø–∫–∞')
    parser.add_argument('-o', '--output', help='–í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª')
    parser.add_argument('-n', '--num-records', type=int, default=200, 
                       help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 200)')
    
    args = parser.parse_args()
    
    processor = DataProcessor()
    
    try:
        if args.command == 'validate':
            processor.validate_log_format(args.input)
            
        elif args.command == 'clean':
            processor.clean_log_data(args.input, args.output)
            
        elif args.command == 'convert':
            processor.convert_google_sheets_export(args.input, args.output)
            
        elif args.command == 'split':
            processor.split_by_events(args.input, args.output)
            
        elif args.command == 'merge':
            # –î–ª—è merge –≤—Ö–æ–¥–Ω–æ–π –ø–∞—Ä–∞–º–µ—Ç—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –ø–∞–ø–∫–æ–π
            input_path = Path(args.input)
            if input_path.is_dir():
                files = list(input_path.glob('*.txt'))
                output_file = args.output or 'merged_log.txt'
                processor.merge_log_files(files, output_file)
            else:
                print("‚ùå –î–ª—è merge —É–∫–∞–∂–∏—Ç–µ –ø–∞–ø–∫—É —Å —Ñ–∞–π–ª–∞–º–∏")
                
        elif args.command == 'sample':
            output_file = args.output or 'sample_data.txt'
            processor.generate_sample_data(output_file, args.num_records)
            
        elif args.command == 'quality':
            processor.analyze_data_quality(args.input)
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–º–∞–Ω–¥—ã: {e}")


if __name__ == "__main__":
    main()