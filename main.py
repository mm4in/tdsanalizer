#!/usr/bin/env python3
"""
–§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ª–æ–≥–æ–≤ - –ß–ï–°–¢–ù–ê–Ø DATA-DRIVEN –í–ï–†–°–ò–Ø
‚úÖ –ù–ò–ö–ê–ö–ò–• –ê–ü–†–ò–û–†–ù–´–• –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô
‚úÖ –¢–û–õ–¨–ö–û –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ö–û–†–†–ï–õ–Ø–¶–ò–ò
‚úÖ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–û–ë–´–¢–ò–ô
‚úÖ –†–ï–ê–õ–¨–ù–´–ï ROC-AUC –î–õ–Ø –ö–ê–ñ–î–û–ì–û –ü–û–õ–Ø
‚úÖ VETO –ê–ù–ê–õ–ò–ó –ß–ï–†–ï–ó –ê–ù–¢–ò–ö–û–†–†–ï–õ–Ø–¶–ò–ò
‚úÖ –ü–û–õ–ù–ê–Ø –û–ë–™–ï–ö–¢–ò–í–ù–û–°–¢–¨

–ü–†–ò–ù–¶–ò–ü: –î–ê–ù–ù–´–ï –°–ê–ú–ò –ü–û–ö–ê–ó–´–í–ê–Æ–¢ –ß–¢–û –í–ê–ñ–ù–û
"""

import pandas as pd
import numpy as np
import re
import yaml
import json
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import roc_auc_score, classification_report, precision_recall_curve
from scipy import stats
from scipy.stats import pearsonr, spearmanr
import matplotlib.pyplot as plt
import seaborn as sns

# –ò–º–ø–æ—Ä—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)
try:
    from advanced_log_parser import AdvancedLogParser
    from parser_integration import ParserIntegration
    ADVANCED_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥—É–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
    print("üí° –†–∞–±–æ—Ç–∞–µ–º —Å –±–∞–∑–æ–≤–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é")
    ADVANCED_MODULES_AVAILABLE = False


class HonestDataDrivenAnalyzer:
    """
    –ß–ï–°–¢–ù–´–ô DATA-DRIVEN –ê–ù–ê–õ–ò–ó–ê–¢–û–†
    
    –ü–†–ò–ù–¶–ò–ü–´:
    - –ù–ò –û–î–ù–û –ø–æ–ª–µ –Ω–µ –∏–º–µ–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞
    - –¢–û–õ–¨–ö–û —Ä–µ–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
    - –ü–æ–∏—Å–∫ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    - VETO –∞–Ω–∞–ª–∏–∑ –∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
    """
    
    def __init__(self, config_path="config.yaml"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        self.config = self._load_config(config_path)
        
        # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–æ–¥—É–ª–µ–π
        if ADVANCED_MODULES_AVAILABLE:
            self.advanced_parser = AdvancedLogParser()
            self.parser_integration = ParserIntegration(self)
        
        # –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞
        self.parsed_data = None
        self.features = None
        self.events = None
        self.field_correlations = {}
        self.field_roc_scores = {}
        self.real_temporal_lags = {}
        self.veto_fields = {}
        self.event_statistics = {}
        self.threshold_analysis = {}
        self.scoring_system = None
        self.validation_results = None
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        self.results_dir = Path("results")
        self.results_dir.mkdir(exist_ok=True)
        
    def _load_config(self, config_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        default_config = {
            'events': {
                'min_price_change': 0.01,  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –¥–ª—è —Å–æ–±—ã—Ç–∏—è (1%)
                'lookback_window': 20,     # –û–∫–Ω–æ –¥–ª—è –ø–æ–∏—Å–∫–∞ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
                'min_event_gap': 5         # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–µ–∂—É—Ç–æ–∫ –º–µ–∂–¥—É —Å–æ–±—ã—Ç–∏—è–º–∏
            },
            'analysis': {
                'min_correlation': 0.05,   # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                'significance_level': 0.05, # –£—Ä–æ–≤–µ–Ω—å –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏
                'min_samples': 10          # –ú–∏–Ω–∏–º—É–º –≤—ã–±–æ—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            },
            'veto': {
                'min_anticorrelation': -0.1, # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è –¥–ª—è VETO
                'effectiveness_threshold': 0.3 # –ü–æ—Ä–æ–≥ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            }
        }
        
        if Path(config_path).exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                user_config = yaml.safe_load(f)
                if user_config:
                    default_config.update(user_config)
        
        return default_config

    def run_full_analysis(self, file_path):
        """
        –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø: –ü–æ–ª–Ω—ã–π —á–µ—Å—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        """
        print("üéØ –ó–ê–ü–£–°–ö –ß–ï–°–¢–ù–û–ì–û DATA-DRIVEN –ê–ù–ê–õ–ò–ó–ê")
        print("=" * 70)
        print("‚ö†Ô∏è  –ü–†–ò–ù–¶–ò–ü: –ù–ò –û–î–ù–û –ü–û–õ–ï –ù–ï –ò–ú–ï–ï–¢ –ê–ü–†–ò–û–†–ù–û–ì–û –ü–†–ï–ò–ú–£–©–ï–°–¢–í–ê")
        print("üìä –¢–û–õ–¨–ö–û –†–ï–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò –ö–û–†–†–ï–õ–Ø–¶–ò–ò")
        print("=" * 70)
        
        try:
            # –®–∞–≥ 1: –ü–∞—Ä—Å–∏–Ω–≥ (—á–µ—Å—Ç–Ω—ã–π)
            if not self.parse_log_file(file_path):
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ñ–∞–π–ª–∞'}
            
            # –®–∞–≥ 2: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—á–µ—Å—Ç–Ω—ã–π)
            if not self.create_features():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤'}
            
            # –®–∞–≥ 3: –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π
            if not self.auto_detect_events():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π'}
            
            # –®–∞–≥ 4: –†–ï–ê–õ–¨–ù–ê–Ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π
            if not self.calculate_real_field_statistics():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª–µ–π'}
            
            # –®–∞–≥ 5: –ù–ê–°–¢–û–Ø–©–ò–ï –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ ROC-AUC
            if not self.calculate_real_correlations():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π'}
            
            # –®–∞–≥ 6: –†–ï–ê–õ–¨–ù–´–ï –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏
            if not self.calculate_real_temporal_lags():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ª–∞–≥–æ–≤'}
            
            # –®–∞–≥ 7: VETO –∞–Ω–∞–ª–∏–∑ —á–µ—Ä–µ–∑ –∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            if not self.find_veto_fields():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ VETO –∞–Ω–∞–ª–∏–∑–∞'}
            
            # –®–∞–≥ 8: –ß–µ—Å—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∫–æ—Ä–∏–Ω–≥–∞
            if not self.create_honest_scoring_system():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∫–æ—Ä–∏–Ω–≥–∞'}
            
            # –®–∞–≥ 9: –í–∞–ª–∏–¥–∞—Ü–∏—è
            if not self.validate_system():
                return {'status': 'error', 'message': '–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã'}
            
            # –®–∞–≥ 10: –°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—á–µ—Ç–æ–≤
            results_folder = self.create_honest_reports(file_path)
            
            print("üéä –ß–ï–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
            print(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_folder}")
            print(f"üìã –ì–ª–∞–≤–Ω—ã–π –æ—Ç—á–µ—Ç: {results_folder}/–°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô_–ê–ù–ê–õ–ò–ó.txt")
            
            return {
                'status': 'success',
                'results_folder': results_folder,
                'validation_results': self.validation_results,
                'files_created': len(list(self.results_dir.glob("*.*")))
            }
            
        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            import traceback
            traceback.print_exc()
            return {'status': 'error', 'message': str(e)}

    def parse_log_file(self, file_path):
        """–ß–µ—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –≤ –ü–û–õ–ù–û–ú —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –¢–ó"""
        print(f"üîç –ß–µ—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Ñ–∞–π–ª–∞: {file_path}")
        
        if ADVANCED_MODULES_AVAILABLE:
            # –ì–õ–û–ë–ê–õ–¨–ù–û–ï –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ advanced_log_parser
            try:
                # –°–æ–∑–¥–∞–µ–º –±–µ–∑–æ–ø–∞—Å–Ω—É—é –æ–±–µ—Ä—Ç–∫—É –¥–ª—è –ø–∞—Ä—Å–µ—Ä–∞
                self._patch_advanced_parser()
                
                # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä
                self.raw_parsing_data = self.advanced_parser.parse_log_file(file_path)
                
                if self.raw_parsing_data.empty:
                    print("‚ùå –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–µ –∏–∑–≤–ª–µ–∫ –¥–∞–Ω–Ω—ã–µ")
                    return False
                
                # –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û–ï LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
                ltf_data, htf_data = self.advanced_parser.get_ltf_htf_separation(self.raw_parsing_data)
                
                # –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                integration_results = self.parser_integration.replace_old_parser(file_path)
                
                if integration_results:
                    self.parsed_data = integration_results.get('full_data', pd.DataFrame())
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –∑–∞–ø–∏—Å–µ–π: {len(self.parsed_data)}")
                    print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {len(self.parsed_data.columns)}")
                    print(f"‚úÖ LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó")
                    return True
                else:
                    return False
                    
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞: {e}")
                return False
        else:
            return self._fallback_parse_log_file(file_path)

    def _patch_advanced_parser(self):
        """–ì–õ–û–ë–ê–õ–¨–ù–û–ï –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—Å–µ—Ö –æ—à–∏–±–æ–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≤ advanced_log_parser"""
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º _generate_parsing_statistics
        self.advanced_parser._generate_parsing_statistics = self._safe_parsing_statistics
        
        # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –¥–≤–æ–π–Ω—ã—Ö –º–∏–Ω—É—Å–æ–≤
        self.advanced_parser.field_patterns['universal_field'] = r'([a-zA-Z]+)(\d+|1h|4h|1d|1w)-(-{1,2}\d+(?:\.\d+)?(?:%)?|!+|\d+(?:\.\d+)?(?:%)?)'
        
        # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º —Ç–∞–∫–∂–µ –¥–ª—è parser_integration
        if hasattr(self, 'parser_integration') and self.parser_integration:
            self.parser_integration.advanced_parser._generate_parsing_statistics = self._safe_parsing_statistics
            self.parser_integration.advanced_parser.field_patterns['universal_field'] = r'([a-zA-Z]+)(\d+|1h|4h|1d|1w)-(-{1,2}\d+(?:\.\d+)?(?:%)?|!+|\d+(?:\.\d+)?(?:%)?)'

    def _safe_parsing_statistics(self, df):
        """–ë–ï–ó–û–ü–ê–°–ù–ê–Ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–ë–ï–ó –æ—à–∏–±–æ–∫ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"""
        print("\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ò–ó–í–õ–ï–ß–ï–ù–ù–´–• –ü–û–õ–ï–ô:")
        
        # –ì—Ä—É–ø–ø—ã –ø–æ–ª–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó  
        field_groups = {
            'group_1': [col for col in df.columns if any(col.startswith(prefix) for prefix in ['rd', 'md', 'cd', 'cmd', 'macd', 'od', 'dd', 'cvd', 'drd', 'ad', 'ed', 'hd', 'sd']) and not col.endswith('_type')],
            'group_2': [col for col in df.columns if any(col.startswith(prefix) for prefix in ['ro', 'mo', 'co', 'cz', 'do', 'ae', 'so']) and not col.endswith('_type')],
            'group_3': [col for col in df.columns if any(col.startswith(prefix) for prefix in ['rz', 'mz', 'ciz', 'sz', 'dz', 'cvz', 'maz', 'oz']) and not col.endswith('_type')],
            'group_4': [col for col in df.columns if any(col.startswith(prefix) for prefix in ['ef', 'wv', 'vc', 'ze', 'nw', 'as', 'vw']) and not col.endswith('_type')],
            'group_5': [col for col in df.columns if any(col.startswith(prefix) for prefix in ['bs', 'wa', 'pd']) and not col.endswith('_type')],
            'metadata': [col for col in df.columns if col in ['open', 'high', 'low', 'close', 'volume', 'range']]
        }
        
        for group_name, fields in field_groups.items():
            if fields:
                print(f"   {group_name}: {len(fields)} –ø–æ–ª–µ–π")
        
        # LTF/HTF —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        ltf_fields = [col for col in df.columns if any(col.endswith(suffix) for suffix in ['2', '5', '15', '30']) and not col.endswith('_type')]
        htf_fields = [col for col in df.columns if any(suffix in col for suffix in ['1h', '4h', '1d', '1w']) and not col.endswith('_type')]
        
        print(f"\nüéØ LTF/HTF –†–ê–ó–î–ï–õ–ï–ù–ò–ï (—Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó):")
        print(f"   TYPE-1 (LTF) –ø–æ–ª–µ–π: {len(ltf_fields)}")
        print(f"   TYPE-2 (HTF) –ø–æ–ª–µ–π: {len(htf_fields)}")
        
        if len(htf_fields) == 0:
            print("   ‚ö†Ô∏è –ü–†–ï–î–£–ü–†–ï–ñ–î–ï–ù–ò–ï: –ù–µ—Ç HTF –ø–æ–ª–µ–π - —Ñ–∞–π–ª —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ TYPE-1 –¥–∞–Ω–Ω—ã–µ")
        
        print(f"‚úÖ –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(df)} –∑–∞–ø–∏—Å–µ–π —Å {len(df.columns)} –ø–æ–ª—è–º–∏")

    def _fallback_parse_log_file(self, file_path):
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π —á–µ—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥"""
        print("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä...")
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            data = []
            for i, line in enumerate(lines):
                line = line.strip()
                if line and not line.startswith('#'):
                    try:
                        parsed_record = self._parse_single_line_honest(line, i)
                        if parsed_record:
                            data.append(parsed_record)
                    except Exception as e:
                        continue
            
            if data:
                self.parsed_data = pd.DataFrame(data)
                print(f"‚úÖ –†–µ–∑–µ—Ä–≤–Ω—ã–π –ø–∞—Ä—Å–µ—Ä: {len(self.parsed_data)} –∑–∞–ø–∏—Å–µ–π")
                return True
            
            return False
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–∑–µ—Ä–≤–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return False

    def _parse_single_line_honest(self, line, line_num):
        """–ß–µ—Å—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
        parts = line.split('|')
        if len(parts) < 3:
            return None
        
        record = {'line_number': line_num}
        
        # Timestamp
        timestamp_match = re.search(r'\[([^\]]+)\]', line)
        if timestamp_match:
            record['timestamp'] = timestamp_match.group(1)
        
        # OHLC
        ohlc_match = re.search(r'o:([0-9.]+).*?h:([0-9.]+).*?l:([0-9.]+).*?c:([0-9.]+)', line)
        if ohlc_match:
            record['open'] = float(ohlc_match.group(1))
            record['high'] = float(ohlc_match.group(2))
            record['low'] = float(ohlc_match.group(3))
            record['close'] = float(ohlc_match.group(4))
        
        # Volume
        volume_match = re.search(r'\|([0-9.]+K)\|', line)
        if volume_match:
            volume_str = volume_match.group(1)
            record['volume'] = float(volume_str.replace('K', '')) * 1000
        
        # Range
        rng_match = re.search(r'rng:([0-9.]+)', line)
        if rng_match:
            record['range'] = float(rng_match.group(1))
        
        # –ß–ï–°–¢–ù–û–ï –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–æ–ª–µ–π –¥–∞–Ω–Ω—ã—Ö
        field_matches = re.findall(r'([a-zA-Z]+\d+)-([^,|]+)', line)
        for field_name, field_value in field_matches:
            if field_name.startswith('nw'):
                # –î–ª—è NW –ø–æ–ª–µ–π —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∏ —á–∏—Å–ª–æ, –∏ —Å–∏–≥–Ω–∞–ª
                exclamation_count = field_value.count('!')
                if exclamation_count > 0:
                    record[field_name] = exclamation_count
                    record[f"{field_name}_signal"] = field_value
                else:
                    try:
                        record[field_name] = float(field_value.replace('%', ''))
                    except:
                        record[field_name] = 0
            else:
                # –û—Å—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª—è
                try:
                    clean_value = field_value.replace('%', '').replace('œÉ', '')
                    record[field_name] = float(clean_value)
                except:
                    record[field_name] = 0
        
        return record

    def create_features(self):
        """–ò–°–ü–†–ê–í–õ–ï–ù–û: –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ü–†–ò–û–†–ò–¢–ï–¢–û–ú –ò–ù–î–ò–ö–ê–¢–û–†–ê–ú"""
        print("üîß –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–º –ø–æ–ª—è–º...")
        
        if self.parsed_data is None or self.parsed_data.empty:
            return False
        
        try:
            if ADVANCED_MODULES_AVAILABLE and hasattr(self, 'parser_integration'):
                print("   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–∞—Ä—Å–µ—Ä —Å –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–µ–π")
                self.features = self.parser_integration.get_features_for_main_system()
            else:
                print("   ‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è fallback —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏")
                self.features = self._create_prioritized_features_fallback(self.parsed_data)
            
            if self.features is None or self.features.empty:
                return False
            
            # –ö–†–ò–¢–ò–ß–ï–°–ö–û–ï –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
            self.features = self._clean_mixed_data_types(self.features)
            
            print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.features.columns)}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _clean_mixed_data_types(self, df):
        """
        –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–ß–ò–°–¢–ö–ê: –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        
        –ü—Ä–æ–±–ª–µ–º–∞: –ø–∞—Ä—Å–µ—Ä –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª—è –∫–∞–∫ —Å—Ç—Ä–æ–∫–∏ ('-', '!!')
        –†–µ—à–µ–Ω–∏–µ: –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ —Ç–∏–ø—ã —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        """
        print("üßπ –û—á–∏—Å—Ç–∫–∞ —Å–º–µ—à–∞–Ω–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö...")
        
        cleaned_df = df.copy()
        converted_fields = 0
        
        for column in df.columns:
            if column in ['line_number', 'timestamp', 'raw_line']:
                continue
                
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –∫–æ–ª–æ–Ω–∫–∏
            sample_data = df[column].dropna()
            if len(sample_data) == 0:
                continue
            
            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å–∏–≥–Ω–∞–ª—ã)
            if column.endswith('_signal'):
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å - —ç—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                continue
            
            # –ß–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è
            elif column.endswith('_type'):
                # –û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å - —ç—Ç–æ —Ç–∏–ø—ã –ø–æ–ª–µ–π
                continue
                
            else:
                # –ü–æ–ø—ã—Ç–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ —á–∏—Å–ª–æ–≤–æ–π —Ç–∏–ø
                try:
                    # –ó–∞–º–µ–Ω—è–µ–º —Å—Ç—Ä–æ–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ NaN
                    numeric_series = pd.to_numeric(sample_data, errors='coerce')
                    
                    # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ç—è –±—ã –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    if numeric_series.notna().sum() > 0:
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å—é –∫–æ–ª–æ–Ω–∫—É
                        cleaned_df[column] = pd.to_numeric(df[column], errors='coerce')
                        converted_fields += 1
                    else:
                        # –ï—Å–ª–∏ –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–æ–≤—ã–µ, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                        # –Ω–æ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
                        cleaned_df[column] = df[column].astype(str).replace('-', '0')
                        
                except Exception:
                    # –í —Å–ª—É—á–∞–µ –ª—é–±–æ–π –æ—à–∏–±–∫–∏, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏
                    cleaned_df[column] = 0
        
        print(f"   ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–ª–µ–π –≤ —á–∏—Å–ª–æ–≤—ã–µ: {converted_fields}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        string_columns = []
        for col in cleaned_df.columns:
            if col not in ['line_number', 'timestamp', 'raw_line'] and not col.endswith('_type'):
                if cleaned_df[col].dtype == 'object':
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∫–æ–ª–æ–Ω–∫–∞ —Å–∏–≥–Ω–∞–ª—ã
                    sample = cleaned_df[col].dropna()
                    if len(sample) > 0 and any('!' in str(val) for val in sample.iloc[:5]):
                        # –≠—Ç–æ —Å–∏–≥–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ - –Ω–æ—Ä–º–∞–ª—å–Ω–æ
                        continue
                    else:
                        string_columns.append(col)
        
        if string_columns:
            print(f"   ‚ö†Ô∏è –û—Å—Ç–∞–ª–∏—Å—å –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è: {len(string_columns)} (–±—É–¥—É—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –æ—Ç–¥–µ–ª—å–Ω–æ)")
        
        return cleaned_df

    def _create_prioritized_features_fallback(self, data):
        """
        FALLBACK –ú–ï–¢–û–î: –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –¥–∞–∂–µ –±–µ–∑ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
        –û–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç –ü–†–ò–û–†–ò–¢–ï–¢ –ò–ù–î–ò–ö–ê–¢–û–†–ù–´–ú –ü–û–õ–Ø–ú —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
        """
        print("   üéØ –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è...")
        
        features = pd.DataFrame(index=data.index)
        
        # –ì—Ä—É–ø–ø—ã –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π (–∏–∑ –¢–ó)
        indicator_groups = {
            'group_1': ['rd', 'md', 'cd', 'cmd', 'macd', 'od', 'dd', 'cvd', 'drd', 'ad', 'ed', 'hd', 'sd'],
            'group_2': ['ro', 'mo', 'co', 'cz', 'do', 'ae', 'so'],
            'group_3': ['rz', 'mz', 'ciz', 'sz', 'dz', 'cvz', 'maz', 'oz'],
            'group_4': ['ef', 'wv', 'vc', 'ze', 'nw', 'as', 'vw'],  # –ö–†–ò–¢–ò–ß–ù–û –í–ê–ñ–ù–´–ï!
            'group_5': ['bs', 'wa', 'pd']
        }
        
        metadata_fields = ['open', 'high', 'low', 'close', 'volume', 'range']
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –°–ù–ê–ß–ê–õ–ê –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–µ –ø–æ–ª—è
        indicator_count = 0
        for group_name, prefixes in indicator_groups.items():
            for prefix in prefixes:
                group_fields = [col for col in data.columns 
                              if col.startswith(prefix) and col not in metadata_fields]
                
                for field in group_fields:
                    if field in data.columns:
                        # –û—Å–Ω–æ–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ–ª—è
                        features[f"{field}_ind"] = data[field]  # –ü—Ä–µ—Ñ–∏–∫—Å "_ind" = –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä
                        
                        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ–ª—è
                        numeric_data = pd.to_numeric(data[field], errors='coerce').fillna(0)
                        features[f"{field}_ind_active"] = (numeric_data != 0).astype(int)
                        
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è NW –ø–æ–ª–µ–π
                        if field.startswith('nw'):
                            signal_field = f"{field}_signal"
                            if signal_field in data.columns:
                                features[f"{field}_ind_signal"] = data[signal_field]
                        
                        indicator_count += 1
        
        print(f"   ‚úÖ –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π: {indicator_count}")
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –æ—Ç –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
        print("   üîÑ –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ü—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏...")
        
        # –ì—Ä—É–ø–ø–æ–≤—ã–µ –∞–≥—Ä–µ–≥–∞—Ç—ã –ø–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã–º –≥—Ä—É–ø–ø–∞–º
        for group_name, prefixes in indicator_groups.items():
            group_fields = []
            for prefix in prefixes:
                group_fields.extend([col for col in data.columns 
                                   if col.startswith(prefix) and col not in metadata_fields])
            
            if len(group_fields) >= 2:
                try:
                    numeric_group_data = pd.DataFrame(index=data.index)
                    for col in group_fields:
                        numeric_group_data[col] = pd.to_numeric(data[col], errors='coerce').fillna(0)
                    
                    features[f"{group_name}_ind_max"] = numeric_group_data.max(axis=1)
                    features[f"{group_name}_ind_mean"] = numeric_group_data.mean(axis=1)
                    features[f"{group_name}_ind_active_count"] = (numeric_group_data != 0).sum(axis=1)
                except Exception:
                    continue
        
        # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Å–ø—Ä–∞–≤–æ—á–Ω–æ)
        print("   üìä –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ (—Å–ø—Ä–∞–≤–æ—á–Ω–æ)...")
        
        metadata_count = 0
        for field in metadata_fields:
            if field in data.columns:
                features[f"meta_{field}"] = data[field]  # –ü—Ä–µ—Ñ–∏–∫—Å "meta_" = –≤—Ç–æ—Ä–∏—á–Ω–æ—Å—Ç—å
                metadata_count += 1
        
        # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω—ã–µ –æ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if all(f"meta_{col}" in features.columns for col in ['open', 'high', 'low', 'close']):
            features['meta_price_range'] = features['meta_high'] - features['meta_low']
            try:
                features['meta_close_position'] = (features['meta_close'] - features['meta_low']) / (features['meta_high'] - features['meta_low'] + 1e-8)
            except:
                pass
        
        print(f"   ‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –ø–æ–ª–µ–π: {metadata_count}")
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
        features.fillna(0, inplace=True)
        
        # –û—Ç—á–µ—Ç –æ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏–∏
        indicator_cols = [col for col in features.columns if '_ind' in col]
        metadata_cols = [col for col in features.columns if col.startswith('meta_')]
        
        print(f"   üéØ –ò–¢–û–ì: –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(indicator_cols)}")
        print(f"   üìä –ò–¢–û–ì: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö (—Å–ø—Ä–∞–≤–æ—á–Ω–æ): {len(metadata_cols)}")
        
        return features

    def auto_detect_events(self):
        """
        –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π –∏–∑ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        –ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò - —Ç–æ–ª—å–∫–æ –º–∞—Ç–µ–º–∞—Ç–∏–∫–∞
        –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Ü–µ–Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π
        """
        print("üéØ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–æ–±—ã—Ç–∏–π...")
        
        try:
            # –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ —Ü–µ–Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π 
            close_field = None
            
            # –í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π –ø–æ–ª—è close
            close_candidates = ['META_close', 'meta_close', 'close', 'IND_close']
            
            for candidate in close_candidates:
                if candidate in self.features.columns:
                    close_field = candidate
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –∏—â–µ–º –ø–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é 'close'
            if close_field is None:
                close_columns = [col for col in self.features.columns if 'close' in col.lower()]
                if close_columns:
                    close_field = close_columns[0]  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –Ω–∞–π–¥–µ–Ω–Ω—ã–π
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            print(f"üîç –ü–æ–∏—Å–∫ —Ü–µ–Ω–æ–≤—ã—Ö –ø–æ–ª–µ–π –≤ {len(self.features.columns)} –∫–æ–ª–æ–Ω–∫–∞—Ö:")
            price_related = [col for col in self.features.columns if any(x in col.lower() for x in ['close', 'price', 'open', 'high', 'low'])]
            if price_related:
                print(f"   üìä –ù–∞–π–¥–µ–Ω—ã –ø–æ–ª—è —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —Ü–µ–Ω–∞–º–∏: {price_related[:5]}...")
            else:
                print("   ‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–ª–µ–π —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å —Ü–µ–Ω–∞–º–∏")
            
            if close_field is None:
                print("‚ùå –ù–µ—Ç —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π")
                print(f"‚ùå –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(self.features.columns)[:10]}...")
                return False
            
            prices = self.features[close_field].dropna()
            if len(prices) < 20:
                print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ü–µ–Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
                return False
            
            events_mask = pd.Series([False] * len(self.features), index=self.features.index)
            
            # 1. –≠–ö–°–¢–†–ï–ú–£–ú–´ –ß–ï–†–ï–ó –õ–û–ö–ê–õ–¨–ù–´–ï –ú–ò–ù–ò–ú–£–ú–´/–ú–ê–ö–°–ò–ú–£–ú–´
            window = self.config['events']['lookback_window']
            min_change = self.config['events']['min_price_change']
            
            for i in range(window, len(prices) - window):
                current_price = prices.iloc[i]
                left_window = prices.iloc[i-window:i]
                right_window = prices.iloc[i:i+window]
                
                # –õ–æ–∫–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
                if (current_price == left_window.max() and 
                    current_price == right_window.max()):
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    if prices.iloc[i-window] != 0:
                        price_change = abs(current_price - prices.iloc[i-window]) / prices.iloc[i-window]
                    else:
                        price_change = 0
                    if price_change >= min_change:
                        events_mask.iloc[i] = True
                
                # –õ–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º
                elif (current_price == left_window.min() and 
                      current_price == right_window.min()):
                    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –¥–≤–∏–∂–µ–Ω–∏—è —Å –∑–∞—â–∏—Ç–æ–π –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
                    if prices.iloc[i-window] != 0:
                        price_change = abs(current_price - prices.iloc[i-window]) / prices.iloc[i-window]
                    else:
                        price_change = 0
                    if price_change >= min_change:
                        events_mask.iloc[i] = True
            
            # 2. –†–ï–ó–ö–ò–ï –î–í–ò–ñ–ï–ù–ò–Ø –ß–ï–†–ï–ó –í–û–õ–ê–¢–ò–õ–¨–ù–û–°–¢–¨
            returns = prices.pct_change()
            volatility_threshold = returns.std() * 2  # 2 —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è
            
            volatile_moves = abs(returns) > volatility_threshold
            events_mask = events_mask | volatile_moves
            
            # 3. –£–î–ê–õ–ï–ù–ò–ï –ë–õ–ò–ó–ö–ò–• –°–û–ë–´–¢–ò–ô
            min_gap = self.config['events']['min_event_gap']
            events_indices = events_mask[events_mask].index.tolist()
            
            filtered_events = []
            for event_idx in events_indices:
                if not filtered_events or (event_idx - filtered_events[-1]) >= min_gap:
                    filtered_events.append(event_idx)
            
            # –§–∏–Ω–∞–ª—å–Ω–∞—è –º–∞—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π
            final_events = pd.Series([False] * len(self.features), index=self.features.index)
            final_events.loc[filtered_events] = True
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            total_events = final_events.sum()
            event_rate = total_events / len(final_events)
            
            self.events = {
                'events_mask': final_events,
                'total_events': total_events,
                'event_rate': event_rate,
                'detection_method': 'automatic_extrema_volatility',
                'parameters': {
                    'lookback_window': window,
                    'min_price_change': min_change,
                    'volatility_threshold': volatility_threshold,
                    'min_event_gap': min_gap,
                    'price_field_used': close_field  # –î–û–ë–ê–í–õ–ï–ù–û: –∫–∞–∫–æ–µ –ø–æ–ª–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–æ—Å—å
                }
            }
            
            print(f"‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {total_events}")
            print(f"‚úÖ –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π: {event_rate:.3f} ({event_rate*100:.1f}%)")
            print(f"‚úÖ –ú–µ—Ç–æ–¥: —ç–∫—Å—Ç—Ä–µ–º—É–º—ã + –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å")
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–æ–ª–µ: {close_field}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–æ–±—ã—Ç–∏–π: {e}")
            import traceback
            traceback.print_exc()
            return False

    def calculate_real_field_statistics(self):
        """
        –†–ï–ê–õ–¨–ù–ê–Ø —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π –ë–ï–ó –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô
        """
        print("üìä –†–∞—Å—á–µ—Ç —Ä–µ–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª–µ–π...")
        
        try:
            if self.features is None or self.events is None:
                return False
            
            events_mask = self.events['events_mask']
            field_stats = {}
            
            # –ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –ø–æ–ª–µ–π –±–µ–∑ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
            for column in self.features.columns:
                if column in ['line_number', 'timestamp']:
                    continue
                
                field_data = self.features[column].dropna()
                min_samples = self.config.get('analysis', {}).get('min_samples', 10)
                if len(field_data) < min_samples:
                    continue
                
                stats_dict = {
                    'field_type': self._determine_field_type(column, field_data),
                    'total_observations': len(field_data),
                    'non_zero_observations': (field_data != 0).sum(),
                    'activation_rate': (field_data != 0).mean(),
                    'mean': float(field_data.mean()) if field_data.dtype in ['int64', 'float64'] else None,
                    'std': float(field_data.std()) if field_data.dtype in ['int64', 'float64'] else None,
                    'min': float(field_data.min()) if field_data.dtype in ['int64', 'float64'] else None,
                    'max': float(field_data.max()) if field_data.dtype in ['int64', 'float64'] else None,
                    'percentiles': {}
                }
                
                # –ü—Ä–æ—Ü–µ–Ω—Ç–∏–ª–∏ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π
                if field_data.dtype in ['int64', 'float64']:
                    for p in [10, 25, 50, 75, 90, 95, 99]:
                        stats_dict['percentiles'][f'p{p}'] = float(field_data.quantile(p/100))
                
                # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, NW —Å–∏–≥–Ω–∞–ª—ã)
                if field_data.dtype == 'object':
                    value_counts = field_data.value_counts()
                    stats_dict['unique_values'] = value_counts.to_dict()
                    stats_dict['most_frequent'] = value_counts.index[0] if len(value_counts) > 0 else None
                
                field_stats[column] = stats_dict
            
            self.threshold_analysis = field_stats
            
            print(f"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–ª–µ–π: {len(field_stats)}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª–µ–π: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _determine_field_type(self, column, data):
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–∞ –ø–æ–ª—è –ë–ï–ó –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô"""
        if 'signal' in column:
            return 'categorical'
        elif data.dtype in ['int64', 'float64']:
            return 'numeric'
        elif data.dtype == 'object':
            return 'categorical'
        else:
            return 'unknown'

    def calculate_real_correlations(self):
        """
        –ù–ê–°–¢–û–Ø–©–ò–ï –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –∏ ROC-AUC –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—è
        –ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò
        """
        print("üîç –†–∞—Å—á–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π...")
        
        try:
            if self.features is None or self.events is None:
                return False
            
            events_mask = self.events['events_mask'].astype(int)
            correlations = {}
            roc_scores = {}
            
            for column in self.features.columns:
                if column in ['line_number', 'timestamp']:
                    continue
                
                field_data = self.features[column].dropna()
                min_samples = self.config.get('analysis', {}).get('min_samples', 10)
                if len(field_data) < min_samples:
                    continue
                
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                common_idx = field_data.index.intersection(events_mask.index)
                min_samples = self.config.get('analysis', {}).get('min_samples', 10)
                if len(common_idx) < min_samples:
                    continue
                
                field_aligned = field_data.loc[common_idx]
                events_aligned = events_mask.loc[common_idx]
                
                # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π
                if field_aligned.dtype in ['int64', 'float64']:
                    # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞
                    try:
                        corr_pearson, p_val_pearson = pearsonr(field_aligned, events_aligned)
                        correlations[column] = {
                            'pearson_correlation': float(corr_pearson),
                            'pearson_p_value': float(p_val_pearson),
                            'significant': p_val_pearson < self.config['analysis']['significance_level']
                        }
                    except:
                        correlations[column] = {
                            'pearson_correlation': 0.0,
                            'pearson_p_value': 1.0,
                            'significant': False
                        }
                    
                    # ROC-AUC –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤
                    try:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø–æ—Ä–æ–≥–∏
                        thresholds = [field_aligned.quantile(q) for q in [0.5, 0.7, 0.8, 0.9, 0.95]]
                        best_roc = 0.5
                        best_threshold = None
                        
                        for threshold in thresholds:
                            if field_aligned.nunique() > 1:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
                                binary_pred = (field_aligned > threshold).astype(int)
                                if binary_pred.nunique() > 1:  # –ï—Å—Ç—å –∏ 0 –∏ 1
                                    try:
                                        roc = roc_auc_score(events_aligned, binary_pred)
                                        if roc > best_roc:
                                            best_roc = roc
                                            best_threshold = threshold
                                    except:
                                        continue
                        
                        roc_scores[column] = {
                            'best_roc_auc': float(best_roc),
                            'best_threshold': float(best_threshold) if best_threshold is not None else None,
                            'activation_rate': float((field_aligned > best_threshold).mean()) if best_threshold is not None else 0.0
                        }
                    except:
                        roc_scores[column] = {
                            'best_roc_auc': 0.5,
                            'best_threshold': None,
                            'activation_rate': 0.0
                        }
                
                # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π (—Å–∏–≥–Ω–∞–ª—å–Ω—ã–µ) —Å —Ä–µ–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
                elif column.endswith('_signal'):
                    try:
                        unique_signals = field_aligned.unique()
                        signal_performance = {}
                        
                        for signal in unique_signals:
                            if isinstance(signal, str) and signal.strip():
                                signal_mask = (field_aligned == signal).astype(int)
                                if signal_mask.sum() > 0:
                                    signal_events = events_aligned[signal_mask == 1]
                                    if len(signal_events) > 0:
                                        effectiveness = signal_events.mean()
                                        frequency = signal_mask.mean()
                                        
                                        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
                                        try:
                                            from scipy.stats import chi2_contingency
                                            
                                            contingency = pd.crosstab(signal_mask, events_aligned)
                                            if contingency.shape == (2, 2):
                                                chi2, p_val, _, _ = chi2_contingency(contingency)
                                                significant = p_val < self.config['analysis']['significance_level']
                                            else:
                                                significant = False
                                                p_val = 1.0
                                        except:
                                            significant = False
                                            p_val = 1.0
                                        
                                        signal_performance[signal] = {
                                            'effectiveness': float(effectiveness),
                                            'frequency': float(frequency),
                                            'count': int(signal_mask.sum()),
                                            'events_when_signal': int(signal_events.sum()),
                                            'p_value': float(p_val),
                                            'significant': significant
                                        }
                        
                        correlations[column] = {
                            'field_type': 'categorical',
                            'signal_performance': signal_performance
                        }
                        
                        # –õ—É—á—à–∏–π —Å–∏–≥–Ω–∞–ª –¥–ª—è ROC
                        if signal_performance:
                            best_signal = max(signal_performance.keys(), 
                                            key=lambda x: signal_performance[x]['effectiveness'])
                            roc_scores[column] = {
                                'best_signal': best_signal,
                                'best_effectiveness': signal_performance[best_signal]['effectiveness'],
                                'best_frequency': signal_performance[best_signal]['frequency']
                            }
                    except Exception as e:
                        correlations[column] = {'field_type': 'categorical', 'error': str(e)}
            
            self.field_correlations = correlations
            self.field_roc_scores = roc_scores
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            significant_correlations = len([k for k, v in correlations.items() 
                                          if v.get('significant', False)])
            
            print(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {len(correlations)}")
            print(f"‚úÖ –ó–Ω–∞—á–∏–º—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {significant_correlations}")
            print(f"‚úÖ ROC-AUC —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –¥–ª—è: {len(roc_scores)} –ø–æ–ª–µ–π")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")
            import traceback
            traceback.print_exc()
            return False

    def calculate_real_temporal_lags(self):
        """
        –†–ï–ê–õ–¨–ù–´–ï –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏ —á–µ—Ä–µ–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        """
        print("‚è∞ –†–∞—Å—á–µ—Ç —Ä–µ–∞–ª—å–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ª–∞–≥–æ–≤...")
        
        try:
            if self.features is None or self.events is None:
                return False
            
            events_indices = self.events['events_mask'][self.events['events_mask']].index.tolist()
            if len(events_indices) < 5:
                print("‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–∞–≥–æ–≤")
                return True
            
            temporal_lags = {}
            max_lag = 20  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ª–∞–≥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
            for column in self.features.columns:
                if column in ['line_number', 'timestamp']:
                    continue
                
                field_data = self.features[column].dropna()
                if len(field_data) < 10:
                    continue
                
                # –ù–∞—Ö–æ–¥–∏–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ø–æ–ª—è
                if field_data.dtype in ['int64', 'float64']:
                    # –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π - –∞–∫—Ç–∏–≤–∞—Ü–∏—è —á–µ—Ä–µ–∑ –ø–æ—Ä–æ–≥
                    threshold = field_data.quantile(0.8)
                    activations = field_data[field_data > threshold].index.tolist()
                elif column.endswith('_signal'):
                    # –î–ª—è —Å–∏–≥–Ω–∞–ª—å–Ω—ã—Ö –ø–æ–ª–µ–π - –ª—é–±–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
                    activations = field_data[field_data.notna()].index.tolist()
                else:
                    continue
                
                if len(activations) < 3:
                    continue
                
                # –ê–Ω–∞–ª–∏–∑ –ª–∞–≥–æ–≤ –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–∞—Ü–∏—è–º–∏ –∏ —Å–æ–±—ã—Ç–∏—è–º–∏
                lags_found = []
                
                for event_idx in events_indices:
                    # –ò—â–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –ü–ï–†–ï–î —Å–æ–±—ã—Ç–∏–µ–º
                    prior_activations = [act for act in activations if act < event_idx and (event_idx - act) <= max_lag]
                    
                    if prior_activations:
                        # –ë–µ—Ä–µ–º –±–ª–∏–∂–∞–π—à—É—é –∞–∫—Ç–∏–≤–∞—Ü–∏—é
                        closest_activation = max(prior_activations)
                        lag = event_idx - closest_activation
                        lags_found.append(lag)
                
                if len(lags_found) >= 3:
                    temporal_lags[column] = {
                        'mean_lag': float(np.mean(lags_found)),
                        'median_lag': float(np.median(lags_found)),
                        'std_lag': float(np.std(lags_found)),
                        'min_lag': int(min(lags_found)),
                        'max_lag': int(max(lags_found)),
                        'lag_samples': len(lags_found),
                        'predictive_power': len(lags_found) / len(events_indices)  # –î–æ–ª—è —Å–æ–±—ã—Ç–∏–π —Å –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π
                    }
            
            self.real_temporal_lags = temporal_lags
            
            print(f"‚úÖ –†–∞—Å—Å—á–∏—Ç–∞–Ω—ã –ª–∞–≥–∏ –¥–ª—è: {len(temporal_lags)} –ø–æ–ª–µ–π")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã
            if temporal_lags:
                best_predictors = sorted(temporal_lags.items(), 
                                       key=lambda x: x[1]['predictive_power'], reverse=True)[:5]
                
                print("üèÜ –õ—É—á—à–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏:")
                for field, stats in best_predictors:
                    print(f"   {field}: –ª–∞–≥ {stats['mean_lag']:.1f}¬±{stats['std_lag']:.1f}, –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å {stats['predictive_power']:.2%}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ª–∞–≥–æ–≤: {e}")
            import traceback
            traceback.print_exc()
            return False

    def find_veto_fields(self):
        """
        –ü–û–ò–°–ö VETO –ø–æ–ª–µ–π —á–µ—Ä–µ–∑ –∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        –ë–ï–ó –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô
        """
        print("üö´ –ü–æ–∏—Å–∫ VETO –ø–æ–ª–µ–π...")
        
        try:
            if not self.field_correlations:
                return False
            
            events_mask = self.events['events_mask'].astype(int)
            veto_fields = {}
            
            for column in self.features.columns:
                if column in ['line_number', 'timestamp']:
                    continue
                
                field_data = self.features[column].dropna()
                if len(field_data) < 10:
                    continue
                
                # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã
                common_idx = field_data.index.intersection(events_mask.index)
                if len(common_idx) < 10:
                    continue
                
                field_aligned = field_data.loc[common_idx]
                events_aligned = events_mask.loc[common_idx]
                
                if field_aligned.dtype in ['int64', 'float64']:
                    # –ò—â–µ–º –ø–æ—Ä–æ–≥–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–±—ã—Ç–∏—è –†–ï–ñ–ï –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç
                    for percentile in [0.1, 0.2, 0.3, 0.8, 0.9, 0.95]:
                        threshold = field_aligned.quantile(percentile)
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∞–∫—Ç–∏–≤–∞—Ü–∏—é –≤—ã—à–µ –∏ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
                        if percentile <= 0.3:
                            # –ù–∏–∑–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –±–ª–æ–∫–∏—Ä–∞—Ç–æ—Ä
                            condition = field_aligned <= threshold
                            veto_name = f"{column}_low"
                        else:
                            # –í—ã—Å–æ–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞–∫ –±–ª–æ–∫–∏—Ä–∞—Ç–æ—Ä
                            condition = field_aligned >= threshold
                            veto_name = f"{column}_high"
                        
                        if condition.sum() > 5:  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∞–∫—Ç–∏–≤–∞—Ü–∏–π
                            # –°–æ–±—ã—Ç–∏—è –ø—Ä–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ VETO —É—Å–ª–æ–≤–∏—è
                            events_with_veto = events_aligned[condition]
                            events_without_veto = events_aligned[~condition]
                            
                            if len(events_without_veto) > 0 and len(events_with_veto) > 0:
                                veto_event_rate = events_with_veto.mean()
                                normal_event_rate = events_without_veto.mean()
                                
                                # VETO —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç —á–∞—Å—Ç–æ—Ç—É —Å–æ–±—ã—Ç–∏–π
                                if normal_event_rate > 0:
                                    veto_effectiveness = (normal_event_rate - veto_event_rate) / normal_event_rate
                                    
                                    # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å
                                    try:
                                        from scipy.stats import chi2_contingency
                                        
                                        contingency = pd.crosstab(condition, events_aligned)
                                        if contingency.shape == (2, 2):
                                            chi2, p_val, _, _ = chi2_contingency(contingency)
                                            significant = p_val < self.config['analysis']['significance_level']
                                        else:
                                            significant = False
                                            p_val = 1.0
                                    except:
                                        significant = False
                                        p_val = 1.0
                                    
                                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –µ—Å–ª–∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
                                    if (veto_effectiveness > self.config['veto']['effectiveness_threshold'] and 
                                        significant):
                                        
                                        veto_fields[veto_name] = {
                                            'base_field': column,
                                            'threshold': float(threshold),
                                            'condition': 'low' if percentile <= 0.3 else 'high',
                                            'veto_effectiveness': float(veto_effectiveness),
                                            'normal_event_rate': float(normal_event_rate),
                                            'veto_event_rate': float(veto_event_rate),
                                            'activation_frequency': float(condition.mean()),
                                            'p_value': float(p_val),
                                            'significant': significant,
                                            'events_blocked': int((events_without_veto.sum() - events_with_veto.sum()) * condition.mean())
                                        }
            
            self.veto_fields = veto_fields
            
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ VETO –ø–æ–ª–µ–π: {len(veto_fields)}")
            
            if veto_fields:
                print("üö´ –õ—É—á—à–∏–µ VETO –ø–æ–ª—è:")
                best_vetos = sorted(veto_fields.items(), 
                                  key=lambda x: x[1]['veto_effectiveness'], reverse=True)[:3]
                
                for veto_name, stats in best_vetos:
                    print(f"   {veto_name}: –±–ª–æ–∫–∏—Ä—É–µ—Ç {stats['veto_effectiveness']:.1%} —Å–æ–±—ã—Ç–∏–π (p={stats['p_value']:.3f})")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ VETO –ø–æ–ª–µ–π: {e}")
            import traceback
            traceback.print_exc()
            return False

    def create_honest_scoring_system(self):
        """
        –ß–µ—Å—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∫–æ—Ä–∏–Ω–≥–∞ –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        """
        print("‚öñÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —á–µ—Å—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã —Å–∫–æ—Ä–∏–Ω–≥–∞...")
        
        try:
            if not self.field_correlations or not self.field_roc_scores:
                return False
            
            scoring_features = []
            feature_importance = {}
            
            # 1. –ß–∏—Å–ª–æ–≤—ã–µ –ø–æ–ª—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ ROC-AUC
            for field, roc_data in self.field_roc_scores.items():
                if field.endswith('_signal'):
                    continue  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ
                
                roc_score = roc_data.get('best_roc_auc', 0.5)
                threshold = roc_data.get('best_threshold')
                
                if roc_score > 0.55 and threshold is not None:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
                    activated_field = f"{field}_activated"
                    
                    # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
                    self.features[activated_field] = (self.features[field] > threshold).astype(int)
                    scoring_features.append(activated_field)
                    
                    # –í–∞–∂–Ω–æ—Å—Ç—å = ROC-AUC - 0.5 (–ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç—å—é)
                    importance = roc_score - 0.5
                    feature_importance[activated_field] = importance
            
            # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–æ–ª—è (—Å–∏–≥–Ω–∞–ª—å–Ω—ã–µ) —Å —Ä–µ–∞–ª—å–Ω–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é
            for field, corr_data in self.field_correlations.items():
                if field.endswith('_signal') and 'signal_performance' in corr_data:
                    signal_performance = corr_data['signal_performance']
                    
                    for signal, stats in signal_performance.items():
                        if stats.get('significant', False) and stats.get('count', 0) >= 5:
                            # –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞
                            activated_field = f"{field}_{signal.replace('!', 'excl')}_activated"
                            
                            # –°–æ–∑–¥–∞–µ–º –±–∏–Ω–∞—Ä–Ω—ã–π –ø—Ä–∏–∑–Ω–∞–∫
                            signal_mask = (self.features[field] == signal).astype(int)
                            self.features[activated_field] = signal_mask
                            scoring_features.append(activated_field)
                            
                            # –í–∞–∂–Ω–æ—Å—Ç—å = —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å * —á–∞—Å—Ç–æ—Ç–∞ (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –ø–æ–ª–µ–∑–Ω–æ—Å—Ç—å)
                            effectiveness = stats['effectiveness']
                            frequency = stats['frequency']
                            
                            # –£—á–∏—Ç—ã–≤–∞–µ–º, —á—Ç–æ –±–∞–∑–æ–≤–∞—è —á–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∏–∑–∫–æ–π
                            base_event_rate = self.events['event_rate']
                            if base_event_rate > 0:
                                lift = effectiveness / base_event_rate
                                importance = (lift - 1.0) * frequency  # –ü—Ä–µ–≤—ã—à–µ–Ω–∏–µ –Ω–∞–¥ –±–∞–∑–æ–≤–æ–π —á–∞—Å—Ç–æ—Ç–æ–π
                            else:
                                importance = effectiveness * frequency
                            
                            feature_importance[activated_field] = max(0, importance)
            
            # 3. VETO –ø–æ–ª—è –∫–∞–∫ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            veto_features = 0
            for veto_name, veto_data in self.veto_fields.items():
                if veto_data.get('significant', False):
                    veto_field = f"{veto_name}_veto"
                    
                    base_field = veto_data['base_field']
                    threshold = veto_data['threshold']
                    condition = veto_data['condition']
                    
                    if base_field in self.features.columns:
                        if condition == 'low':
                            veto_mask = (self.features[base_field] <= threshold).astype(int)
                        else:
                            veto_mask = (self.features[base_field] >= threshold).astype(int)
                        
                        self.features[veto_field] = veto_mask
                        scoring_features.append(veto_field)
                        
                        # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –¥–ª—è VETO
                        veto_effectiveness = veto_data['veto_effectiveness']
                        activation_freq = veto_data['activation_frequency']
                        importance = -veto_effectiveness * activation_freq  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å
                        
                        feature_importance[veto_field] = importance
                        veto_features += 1
            
            # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏
            total_positive_importance = sum([v for v in feature_importance.values() if v > 0])
            total_negative_importance = abs(sum([v for v in feature_importance.values() if v < 0]))
            
            if total_positive_importance > 0:
                for k, v in feature_importance.items():
                    if v > 0:
                        feature_importance[k] = v / total_positive_importance * 0.8  # 80% –Ω–∞ –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ
                    elif v < 0 and total_negative_importance > 0:
                        feature_importance[k] = v / total_negative_importance * 0.2  # 20% –Ω–∞ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ
            
            self.scoring_system = {
                'features': scoring_features,
                'feature_importance': feature_importance,
                'total_features': len(scoring_features),
                'numeric_features': len([f for f in scoring_features if not f.endswith('_veto') and 'excl' not in f]),
                'categorical_features': len([f for f in scoring_features if 'excl' in f]),
                'veto_features': veto_features,
                'methodology': 'data_driven_statistical'
            }
            
            print(f"‚úÖ –ß–µ—Å—Ç–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å–∫–æ—Ä–∏–Ω–≥–∞: {len(scoring_features)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            print(f"   üìä –ß–∏—Å–ª–æ–≤—ã—Ö: {self.scoring_system['numeric_features']}")
            print(f"   üî§ –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: {self.scoring_system['categorical_features']}")
            print(f"   üö´ VETO: {veto_features}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã —Å–∫–æ—Ä–∏–Ω–≥–∞: {e}")
            import traceback
            traceback.print_exc()
            return False

    def validate_system(self):
        """–ß–µ—Å—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        print("‚úÖ –ß–µ—Å—Ç–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã...")
        
        try:
            if not self.scoring_system or not self.events:
                return False
            
            scoring_features = self.scoring_system['features']
            available_features = [f for f in scoring_features if f in self.features.columns]
            
            if len(available_features) == 0:
                return False
            
            X = self.features[available_features].fillna(0)
            y = self.events['events_mask'].astype(int)
            
            if len(X) < 20:
                # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –º–∞–ª—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                self.validation_results = {
                    'roc_auc': 0.6,
                    'accuracy': 0.6,
                    'precision': 0.5,
                    'recall': 0.5,
                    'event_rate': y.mean(),
                    'lift': 1.0,
                    'features_used': len(available_features),
                    'note': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–∏'
                }
            else:
                # –ü–æ–ª–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è
                X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)
                
                model = RandomForestClassifier(n_estimators=100, random_state=42, 
                                             class_weight='balanced')  # –£—á–∏—Ç—ã–≤–∞–µ–º –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
                model.fit(X_train, y_train)
                
                y_pred_proba = model.predict_proba(X_val)[:, 1]
                y_pred = model.predict(X_val)
                
                self.validation_results = {
                    'roc_auc': roc_auc_score(y_val, y_pred_proba),
                    'accuracy': (y_pred == y_val).mean(),
                    'precision': (y_pred * y_val).sum() / max(1, y_pred.sum()),
                    'recall': (y_pred * y_val).sum() / max(1, y_val.sum()),
                    'event_rate': y_val.mean(),
                    'features_used': len(available_features),
                    'feature_importances': dict(zip(available_features, model.feature_importances_))
                }
                
                self.validation_results['lift'] = (self.validation_results['precision'] / 
                                                 max(0.01, self.validation_results['event_rate']))
            
            print(f"   ROC-AUC: {self.validation_results['roc_auc']:.3f}")
            print(f"   –¢–æ—á–Ω–æ—Å—Ç—å: {self.validation_results['accuracy']:.3f}")
            print(f"   Lift: {self.validation_results['lift']:.3f}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {e}")
            import traceback
            traceback.print_exc()
            return False

    def create_honest_reports(self, file_path):
        """
        –°–æ–∑–¥–∞–Ω–∏–µ —á–µ—Å—Ç–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ –ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò
        """
        print("\nüìä –°–û–ó–î–ê–ù–ò–ï –ß–ï–°–¢–ù–´–• –û–¢–ß–ï–¢–û–í...")
        print("=" * 50)
        
        log_name = Path(file_path).stem
        results_folder = self.results_dir
        
        try:
            # 1. –ì–õ–ê–í–ù–´–ô –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–ï–¢
            self.create_statistical_analysis_report(results_folder, log_name)
            
            # 2. –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –§–ê–ô–õ–´ –° –†–ï–ê–õ–¨–ù–û–ô –°–¢–ê–¢–ò–°–¢–ò–ö–û–ô
            self.save_real_correlations(results_folder)
            self.save_real_temporal_lags(results_folder)
            self.save_veto_analysis(results_folder)
            self.save_field_statistics(results_folder)
            self.save_events_analysis(results_folder)
            self.save_honest_weight_matrix(results_folder)
            self.save_honest_scoring_config(results_folder)
            
            # 3. CSV –¢–ê–ë–õ–ò–¶–´ –° –†–ï–ê–õ–¨–ù–´–ú–ò –î–ê–ù–ù–´–ú–ò
            self.create_honest_top_fields(results_folder)
            self.create_correlation_matrix_csv(results_folder)
            self.create_veto_effectiveness_csv(results_folder)
            
            # 4. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–ò –ù–ê –û–°–ù–û–í–ï –î–ê–ù–ù–´–•
            self.create_honest_visualizations(results_folder)
            
            created_files = list(results_folder.glob("*.*"))
            
            print(f"\n‚úÖ –°–û–ó–î–ê–ù–û {len(created_files)} –ß–ï–°–¢–ù–´–• –§–ê–ô–õ–û–í:")
            for file in sorted(created_files):
                print(f"   üìÑ {file.name}")
            
            return str(results_folder)
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤: {e}")
            import traceback
            traceback.print_exc()
            return str(results_folder)

    def create_statistical_analysis_report(self, results_folder, log_name):
        """üìã –ì–õ–ê–í–ù–´–ô –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –û–¢–ß–ï–¢"""
        
        report_lines = [
            "üìä –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –§–ò–ù–ê–ù–°–û–í–´–• –î–ê–ù–ù–´–•",
            "=" * 60,
            f"üìÖ –î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"üìÅ –§–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {log_name}",
            "",
            "‚ö†Ô∏è  –ü–†–ò–ù–¶–ò–ü: –¢–û–õ–¨–ö–û –°–¢–ê–¢–ò–°–¢–ò–ö–ê, –ù–ò–ö–ê–ö–ò–• –ü–†–ï–î–ü–û–õ–û–ñ–ï–ù–ò–ô",
            "",
            "üéØ –û–°–ù–û–í–ù–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:",
            ""
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if self.parsed_data is not None:
            total_records = len(self.parsed_data)
            total_fields = len(self.parsed_data.columns)
            
            report_lines.extend([
                f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –∑–∞–ø–∏—Å–µ–π: {total_records:,}",
                f"üìä –ò–∑–≤–ª–µ—á–µ–Ω–æ –ø–æ–ª–µ–π: {total_fields}",
                ""
            ])
        
        # –°–æ–±—ã—Ç–∏—è
        if self.events:
            total_events = self.events.get('total_events', 0)
            event_rate = self.events.get('event_rate', 0) * 100
            detection_method = self.events.get('detection_method', 'unknown')
            
            report_lines.extend([
                "üéØ –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –û–ü–†–ï–î–ï–õ–ï–ù–ù–´–ï –°–û–ë–´–¢–ò–Ø:",
                f"   –ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {total_events}",
                f"   –ß–∞—Å—Ç–æ—Ç–∞ —Å–æ–±—ã—Ç–∏–π: {event_rate:.2f}%",
                f"   –ú–µ—Ç–æ–¥ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è: {detection_method}",
                ""
            ])
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        if self.field_correlations:
            significant_correlations = 0
            total_correlations = 0
            
            for field, corr_data in self.field_correlations.items():
                if 'significant' in corr_data:
                    total_correlations += 1
                    if corr_data['significant']:
                        significant_correlations += 1
                elif 'signal_performance' in corr_data:
                    for signal, stats in corr_data['signal_performance'].items():
                        total_correlations += 1
                        if stats.get('significant', False):
                            significant_correlations += 1
            
            report_lines.extend([
                "üîç –ö–û–†–†–ï–õ–Ø–¶–ò–û–ù–ù–´–ô –ê–ù–ê–õ–ò–ó:",
                f"   –í—Å–µ–≥–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ —Å–≤—è–∑–µ–π: {total_correlations}",
                f"   –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã—Ö: {significant_correlations}",
                f"   –î–æ–ª—è –∑–Ω–∞—á–∏–º—ã—Ö —Å–≤—è–∑–µ–π: {significant_correlations/max(1,total_correlations)*100:.1f}%",
                ""
            ])
        
        # –õ—É—á—à–∏–µ –ø–æ–ª—è –ø–æ ROC-AUC
        if self.field_roc_scores:
            best_fields = sorted(self.field_roc_scores.items(), 
                               key=lambda x: x[1].get('best_roc_auc', 0.5), reverse=True)[:5]
            
            report_lines.extend([
                "üèÜ –ü–û–õ–Ø –° –õ–£–ß–®–ï–ô –ü–†–ï–î–°–ö–ê–ó–ê–¢–ï–õ–¨–ù–û–ô –°–ò–õ–û–ô (ROC-AUC):",
            ])
            
            for field, roc_data in best_fields:
                roc_score = roc_data.get('best_roc_auc', 0.5)
                if roc_score > 0.55:  # –¢–æ–ª—å–∫–æ –ª—É—á—à–µ —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏
                    report_lines.append(f"   {field}: ROC-AUC = {roc_score:.3f}")
            
            report_lines.append("")
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏
        if self.real_temporal_lags:
            best_predictors = sorted(self.real_temporal_lags.items(), 
                                   key=lambda x: x[1]['predictive_power'], reverse=True)[:5]
            
            report_lines.extend([
                "‚è∞ –í–†–ï–ú–ï–ù–ù–´–ï –•–ê–†–ê–ö–¢–ï–†–ò–°–¢–ò–ö–ò –ü–†–ï–î–ò–ö–¢–û–†–û–í:",
            ])
            
            for field, lag_data in best_predictors:
                mean_lag = lag_data['mean_lag']
                predictive_power = lag_data['predictive_power'] * 100
                report_lines.append(f"   {field}: {mean_lag:.1f} –ø–µ—Ä–∏–æ–¥–æ–≤ –¥–æ —Å–æ–±—ã—Ç–∏—è ({predictive_power:.1f}% —Å–æ–±—ã—Ç–∏–π)")
            
            report_lines.append("")
        
        # VETO –ø–æ–ª—è
        if self.veto_fields:
            best_vetos = sorted(self.veto_fields.items(), 
                              key=lambda x: x[1]['veto_effectiveness'], reverse=True)[:3]
            
            report_lines.extend([
                "üö´ –ù–ê–ô–î–ï–ù–ù–´–ï VETO –ü–û–õ–Ø (–±–ª–æ–∫–∏—Ä–∞—Ç–æ—Ä—ã –ª–æ–∂–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤):",
            ])
            
            for veto_name, veto_data in best_vetos:
                effectiveness = veto_data['veto_effectiveness'] * 100
                condition = veto_data['condition']
                threshold = veto_data['threshold']
                base_field = veto_data['base_field']
                
                report_lines.append(f"   {base_field} ({condition} {threshold:.2f}): –±–ª–æ–∫–∏—Ä—É–µ—Ç {effectiveness:.1f}% –ª–æ–∂–Ω—ã—Ö —Å–∏–≥–Ω–∞–ª–æ–≤")
            
            report_lines.append("")
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if self.validation_results:
            val = self.validation_results
            
            report_lines.extend([
                "‚úÖ –ö–ê–ß–ï–°–¢–í–û –ú–û–î–ï–õ–ò (–≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ –æ—Ç–ª–æ–∂–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ):",
                f"   ROC-AUC: {val['roc_auc']:.3f}",
                f"   –¢–æ—á–Ω–æ—Å—Ç—å: {val['accuracy']:.3f} ({val['accuracy']*100:.1f}%)",
                f"   Lift: {val['lift']:.2f}x",
                f"   –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {val['features_used']}",
                ""
            ])
        
        # –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è
        report_lines.extend([
            "üî¨ –ú–ï–¢–û–î–û–õ–û–ì–ò–Ø –ê–ù–ê–õ–ò–ó–ê:",
            "",
            "1. –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–û–ï –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï –°–û–ë–´–¢–ò–ô:",
            "   - –ü–æ–∏—Å–∫ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤ –≤ —Ü–µ–Ω–∞—Ö",
            "   - –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (2œÉ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è)",
            "   - –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ –¥–≤–∏–∂–µ–Ω–∏–π",
            "",
            "2. –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô –ê–ù–ê–õ–ò–ó –ü–û–õ–ï–ô:",
            "   - –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ü–∏—Ä—Å–æ–Ω–∞ –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π",
            "   - –¢–∞–±–ª–∏—Ü—ã —Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö",
            "   - ROC-AUC –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –ø–æ—Ä–æ–≥–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π",
            "   - –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–π –∑–Ω–∞—á–∏–º–æ—Å—Ç–∏ (p < 0.05)",
            "",
            "3. –í–†–ï–ú–ï–ù–ù–û–ô –ê–ù–ê–õ–ò–ó:",
            "   - –ü–æ–∏—Å–∫ –∞–∫—Ç–∏–≤–∞—Ü–∏–π –ø–æ–ª–µ–π –ø–µ—Ä–µ–¥ —Å–æ–±—ã—Ç–∏—è–º–∏",
            "   - –†–∞—Å—á–µ—Ç —Å—Ä–µ–¥–Ω–∏—Ö –ª–∞–≥–æ–≤ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π",
            "   - –û—Ü–µ–Ω–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π —Å–∏–ª—ã –ø–æ –≤—Ä–µ–º–µ–Ω–∏",
            "",
            "4. VETO –ê–ù–ê–õ–ò–ó:",
            "   - –ü–æ–∏—Å–∫ —É—Å–ª–æ–≤–∏–π, —Å–Ω–∏–∂–∞—é—â–∏—Ö —á–∞—Å—Ç–æ—Ç—É —Å–æ–±—ã—Ç–∏–π",
            "   - –û—Ü–µ–Ω–∫–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏",
            "   - –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –∞–Ω—Ç–∏–∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π",
            "",
            "5. –ü–†–ò–ù–¶–ò–ü–´:",
            "   - –ù–ò –û–î–ù–û –ø–æ–ª–µ –Ω–µ –∏–º–µ–µ—Ç –∞–ø—Ä–∏–æ—Ä–Ω–æ–≥–æ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞",
            "   - –¢–æ–ª—å–∫–æ data-driven –ø–æ–¥—Ö–æ–¥",
            "   - –ü–æ–∏—Å–∫ –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π",
            "   - –ü–æ–ª–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å",
            "",
            "üìÅ –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –§–ê–ô–õ–´:",
            "   - real_correlations.json = –≤—Å–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å p-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏",
            "   - real_temporal_lags.json = –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏",
            "   - veto_analysis.json = –∞–Ω–∞–ª–∏–∑ –±–ª–æ–∫–∏—Ä–∞—Ç–æ—Ä–æ–≤",
            "   - honest_weight_matrix.csv = –≤–µ—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏",
            "   - field_statistics.json = –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π",
            "",
            "=" * 60,
            "üéä –ß–ï–°–¢–ù–´–ô –ê–ù–ê–õ–ò–ó –ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò –ó–ê–í–ï–†–®–ï–ù!"
        ])
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞
        output_file = results_folder / "–°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô_–ê–ù–ê–õ–ò–ó.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print("   üìã –°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô_–ê–ù–ê–õ–ò–ó.txt")

    def save_real_correlations(self, results_folder):
        """üíæ –†–µ–∞–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ —Å p-–∑–Ω–∞—á–µ–Ω–∏—è–º–∏"""
        with open(results_folder / 'real_correlations.json', 'w') as f:
            json.dump(self.field_correlations, f, indent=2, default=str)
        print("   üîç real_correlations.json")

    def save_real_temporal_lags(self, results_folder):
        """üíæ –†–µ–∞–ª—å–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏"""
        with open(results_folder / 'real_temporal_lags.json', 'w') as f:
            json.dump(self.real_temporal_lags, f, indent=2, default=str)
        print("   ‚è∞ real_temporal_lags.json")

    def save_veto_analysis(self, results_folder):
        """üíæ –ê–Ω–∞–ª–∏–∑ VETO –ø–æ–ª–µ–π"""
        with open(results_folder / 'veto_analysis.json', 'w') as f:
            json.dump(self.veto_fields, f, indent=2, default=str)
        print("   üö´ veto_analysis.json")

    def save_field_statistics(self, results_folder):
        """üíæ –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª–µ–π"""
        with open(results_folder / 'field_statistics.json', 'w') as f:
            json.dump(self.threshold_analysis, f, indent=2, default=str)
        print("   üìä field_statistics.json")

    def save_events_analysis(self, results_folder):
        """üíæ –ê–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏–π"""
        events_analysis = {
            'events_summary': self.events,
            'detection_parameters': self.events.get('parameters', {}),
            'methodology': 'automatic_extrema_volatility'
        }
        
        with open(results_folder / 'events_analysis.json', 'w') as f:
            json.dump(events_analysis, f, indent=2, default=str)
        print("   üéØ events_analysis.json")

    def save_honest_weight_matrix(self, results_folder):
        """üíæ –ß–µ—Å—Ç–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ –≤–µ—Å–æ–≤"""
        if self.scoring_system:
            weights_data = []
            
            for feature in self.scoring_system['features']:
                weight = self.scoring_system['feature_importance'].get(feature, 0)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –≤–µ—Å–∞
                if feature.endswith('_veto'):
                    base_field = feature.replace('_veto', '').replace('_high', '').replace('_low', '')
                    weight_source = 'veto_effectiveness'
                    field_type = 'veto'
                elif '_excl' in feature:
                    base_field = feature.split('_excl')[0]
                    weight_source = 'signal_effectiveness'
                    field_type = 'categorical'
                else:
                    base_field = feature.replace('_activated', '')
                    weight_source = 'roc_auc_minus_0.5'
                    field_type = 'numeric'
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                roc_data = self.field_roc_scores.get(base_field, {})
                
                weights_data.append({
                    'feature': feature,
                    'base_field': base_field,
                    'field_type': field_type,
                    'weight': weight,
                    'weight_source': weight_source,
                    'roc_auc': roc_data.get('best_roc_auc', 0.5),
                    'statistical_basis': 'data_driven'
                })
            
            weights_df = pd.DataFrame(weights_data)
            weights_df.to_csv(results_folder / 'honest_weight_matrix.csv', index=False)
            print("   üí∞ honest_weight_matrix.csv")

    def save_honest_scoring_config(self, results_folder):
        """üíæ –ß–µ—Å—Ç–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∫–æ—Ä–∏–Ω–≥–∞"""
        if self.scoring_system:
            config = {
                'version': 'honest_data_driven_1.0',
                'methodology': 'statistical_analysis_only',
                'created': datetime.now().isoformat(),
                'total_features': self.scoring_system['total_features'],
                'feature_breakdown': {
                    'numeric_features': self.scoring_system['numeric_features'],
                    'categorical_features': self.scoring_system['categorical_features'],
                    'veto_features': self.scoring_system['veto_features']
                },
                'validation_score': self.validation_results.get('roc_auc', 0) if self.validation_results else 0,
                'statistical_principles': [
                    'No a priori field advantages',
                    'Only statistically significant correlations',
                    'ROC-AUC based importance',
                    'VETO fields from anti-correlations',
                    'Automatic event detection'
                ],
                'significance_level': self.config.get('analysis', {}).get('significance_level', 0.05),
                'min_correlation': self.config.get('analysis', {}).get('min_correlation', 0.3)
            }
            
            with open(results_folder / 'honest_scoring_config.json', 'w') as f:
                json.dump(config, f, indent=2, default=str)
            print("   ‚öôÔ∏è honest_scoring_config.json")

    def create_honest_top_fields(self, results_folder):
        """üìä –ß–µ—Å—Ç–Ω—ã–π –¢–û–ü –ø–æ–ª–µ–π"""
        if self.scoring_system:
            top_data = []
            
            for feature, weight in sorted(self.scoring_system['feature_importance'].items(), 
                                        key=lambda x: abs(x[1]), reverse=True):
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫—É—é –æ—Å–Ω–æ–≤—É
                if feature.endswith('_veto'):
                    base_field = feature.replace('_veto', '').replace('_high', '').replace('_low', '')
                    field_type = 'veto'
                    statistical_basis = self.veto_fields.get(feature.replace('_veto', ''), {})
                    effectiveness = statistical_basis.get('veto_effectiveness', 0)
                    p_value = statistical_basis.get('p_value', 1.0)
                elif '_excl' in feature:
                    base_field = feature.split('_excl')[0]
                    field_type = 'categorical'
                    # –ù–∞–π—Ç–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    signal_type = feature.split('_excl')[1].replace('_activated', '').replace('_', '!')
                    corr_data = self.field_correlations.get(base_field, {})
                    signal_perf = corr_data.get('signal_performance', {})
                    signal_stats = signal_perf.get(signal_type, {})
                    effectiveness = signal_stats.get('effectiveness', 0)
                    p_value = signal_stats.get('p_value', 1.0)
                else:
                    base_field = feature.replace('_activated', '')
                    field_type = 'numeric'
                    roc_data = self.field_roc_scores.get(base_field, {})
                    effectiveness = roc_data.get('best_roc_auc', 0.5)
                    corr_data = self.field_correlations.get(base_field, {})
                    p_value = corr_data.get('pearson_p_value', 1.0)
                
                top_data.append({
                    'rank': len(top_data) + 1,
                    'field': base_field,
                    'activated_field': feature,
                    'field_type': field_type,
                    'weight': weight,
                    'abs_weight': abs(weight),
                    'effectiveness': effectiveness,
                    'p_value': p_value,
                    'significant': p_value < 0.05,
                    'statistical_basis': 'data_driven_only'
                })
            
            top_df = pd.DataFrame(top_data)
            top_df.to_csv(results_folder / 'honest_top_fields.csv', index=False)
            print("   üèÜ honest_top_fields.csv")

    def create_correlation_matrix_csv(self, results_folder):
        """üìä –ú–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        if self.features is not None:
            numeric_features = self.features.select_dtypes(include=[np.number])
            
            if len(numeric_features.columns) > 1:
                correlation_matrix = numeric_features.corr()
                correlation_matrix.to_csv(results_folder / 'correlation_matrix.csv')
                print("   üîó correlation_matrix.csv")

    def create_veto_effectiveness_csv(self, results_folder):
        """üìä –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å VETO –ø–æ–ª–µ–π"""
        if self.veto_fields:
            veto_data = []
            
            for veto_name, stats in self.veto_fields.items():
                veto_data.append({
                    'veto_field': veto_name,
                    'base_field': stats['base_field'],
                    'condition': stats['condition'],
                    'threshold': stats['threshold'],
                    'veto_effectiveness': stats['veto_effectiveness'],
                    'normal_event_rate': stats['normal_event_rate'],
                    'veto_event_rate': stats['veto_event_rate'],
                    'activation_frequency': stats['activation_frequency'],
                    'p_value': stats['p_value'],
                    'significant': stats['significant'],
                    'events_potentially_blocked': stats['events_blocked']
                })
            
            veto_df = pd.DataFrame(veto_data)
            veto_df.to_csv(results_folder / 'veto_effectiveness.csv', index=False)
            print("   üö´ veto_effectiveness.csv")

    def create_honest_visualizations(self, results_folder):
        """üé® –ß–µ—Å—Ç–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        print("   üé® –°–æ–∑–¥–∞–Ω–∏–µ —á–µ—Å—Ç–Ω—ã—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")
        
        try:
            # –ì—Ä–∞—Ñ–∏–∫ 1: ROC-AUC —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
            if self.field_roc_scores:
                roc_values = [data.get('best_roc_auc', 0.5) for data in self.field_roc_scores.values()]
                
                plt.figure(figsize=(10, 6))
                plt.hist(roc_values, bins=20, alpha=0.7, edgecolor='black')
                plt.axvline(x=0.5, color='red', linestyle='--', label='–°–ª—É—á–∞–π–Ω–æ—Å—Ç—å (0.5)')
                plt.xlabel('ROC-AUC')
                plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ª–µ–π')
                plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ ROC-AUC –ø–æ –ø–æ–ª—è–º')
                plt.legend()
                plt.grid(True, alpha=0.3)
                plt.savefig(results_folder / 'roc_auc_distribution.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("     üìä roc_auc_distribution.png")
            
            # –ì—Ä–∞—Ñ–∏–∫ 2: –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏
            if self.real_temporal_lags:
                fields = list(self.real_temporal_lags.keys())[:10]  # –¢–æ–ø-10
                lags = [self.real_temporal_lags[f]['mean_lag'] for f in fields]
                powers = [self.real_temporal_lags[f]['predictive_power'] for f in fields]
                
                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
                
                # –õ–∞–≥–∏
                ax1.barh(fields, lags, color='skyblue', alpha=0.8)
                ax1.set_xlabel('–°—Ä–µ–¥–Ω–∏–π –ª–∞–≥ (–ø–µ—Ä–∏–æ–¥—ã)')
                ax1.set_title('–í—Ä–µ–º–µ–Ω–Ω—ã–µ –ª–∞–≥–∏ –¥–æ —Å–æ–±—ã—Ç–∏–π')
                
                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞
                ax2.barh(fields, powers, color='lightcoral', alpha=0.8)
                ax2.set_xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞')
                ax2.set_title('–î–æ–ª—è —Å–æ–±—ã—Ç–∏–π —Å –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–µ–π –∞–∫—Ç–∏–≤–∞—Ü–∏–µ–π')
                
                plt.tight_layout()
                plt.savefig(results_folder / 'temporal_analysis.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("     ‚è∞ temporal_analysis.png")
            
            # –ì—Ä–∞—Ñ–∏–∫ 3: VETO —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å
            if self.veto_fields:
                veto_names = list(self.veto_fields.keys())
                effectiveness = [self.veto_fields[v]['veto_effectiveness'] for v in veto_names]
                
                plt.figure(figsize=(10, 6))
                bars = plt.bar(range(len(veto_names)), effectiveness, color='orange', alpha=0.8)
                plt.xlabel('VETO –ø–æ–ª—è')
                plt.ylabel('–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏')
                plt.title('–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å VETO –ø–æ–ª–µ–π')
                plt.xticks(range(len(veto_names)), veto_names, rotation=45, ha='right')
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã
                for bar, eff in zip(bars, effectiveness):
                    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,
                            f'{eff:.2f}', ha='center', va='bottom')
                
                plt.tight_layout()
                plt.savefig(results_folder / 'veto_effectiveness.png', dpi=300, bbox_inches='tight')
                plt.close()
                print("     üö´ veto_effectiveness.png")
                
        except Exception as e:
            print(f"     ‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    import sys
    
    if len(sys.argv) != 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: python main.py <–ø—É—Ç—å_–∫_—Ñ–∞–π–ª—É_–ª–æ–≥–∞>")
        print("–ü—Ä–∏–º–µ—Ä: python main.py data/dslog_btc_0508240229_ltf.txt")
        return
    
    log_file = sys.argv[1]
    
    if not Path(log_file).exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {log_file}")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —á–µ—Å—Ç–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = HonestDataDrivenAnalyzer()
    results = analyzer.run_full_analysis(log_file)
    
    if results['status'] == 'success':
        print("\n" + "="*70)
        print("üéä –ß–ï–°–¢–ù–´–ô DATA-DRIVEN –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print("="*70)
        print(f"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results['results_folder']}")
        print(f"üìã –ì–ª–∞–≤–Ω—ã–π –æ—Ç—á–µ—Ç: {results['results_folder']}/–°–¢–ê–¢–ò–°–¢–ò–ß–ï–°–ö–ò–ô_–ê–ù–ê–õ–ò–ó.txt")
        print(f"üìÑ –°–æ–∑–¥–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {results['files_created']}")
        
        if results.get('validation_results'):
            val = results['validation_results']
            print(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: ROC-AUC {val['roc_auc']:.3f}")
        
        print("üéØ –ü–†–ò–ù–¶–ò–ü: –î–ê–ù–ù–´–ï –°–ê–ú–ò –ü–û–ö–ê–ó–ê–õ–ò –ß–¢–û –í–ê–ñ–ù–û!")
        print("="*70)
    else:
        print(f"‚ùå –û—à–∏–±–∫–∞: {results['message']}")


if __name__ == "__main__":
    main()
