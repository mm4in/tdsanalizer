#!/usr/bin/env python3
"""
–ü–†–ê–í–ò–õ–¨–ù–´–ô DATA-DRIVEN –ê–ù–ê–õ–ò–ó–ê–¢–û–† - –ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò!

–¶–µ–ª—å: –ù–∞–π—Ç–∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–∞–º–ø/–ø–∞–º–ø —Å–æ–±—ã—Ç–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –°–¢–ê–¢–ò–°–¢–ò–ö–ò, –∞ –Ω–µ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π!

–¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π:
1. –õ–û–ò —Å –æ—Ç–∫–∞—Ç–∞–º–∏ –í–í–ï–†–• 3%+ (–¥–∞–º–ø ‚Üí –∫–æ–Ω—Ç—Ä—Ç—Ä–µ–Ω–¥ –ø–æ–∫—É–ø–∫–∞)
2. –•–ê–ò —Å –æ—Ç–∫–∞—Ç–∞–º–∏ –í–ù–ò–ó 3%+ (–ø–∞–º–ø ‚Üí –∫–æ–Ω—Ç—Ä—Ç—Ä–µ–Ω–¥ –ø—Ä–æ–¥–∞–∂–∞)  
3. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –¥–∞–º–ø–∞ (–±–µ–∑ –æ—Ç–∫–∞—Ç–æ–≤)
4. –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –ø–∞–º–ø–∞ (–±–µ–∑ –æ—Ç–∫–∞—Ç–æ–≤)

DATA-DRIVEN: –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï –ø–æ–ª—è –±–µ–∑ –∞–ø—Ä–∏–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥–ø–æ–ª–æ–∂–µ–Ω–∏–π!
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Optional
from collections import defaultdict
import re
import json
from pathlib import Path
from datetime import datetime

class TrueDataDrivenAnalyzer:
    """–ü–†–ê–í–ò–õ–¨–ù–´–ô –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –±–µ–∑ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏"""
    
    def __init__(self):
        self.all_data = []
        self.all_fields_stats = {}
        self.events = []
        self.field_correlations = {}
        
    def parse_line_all_fields(self, line: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –ø–æ–ª–µ–π –±–µ–∑ –ø—Ä–µ–¥–≤–∑—è—Ç–æ—Å—Ç–∏"""
        fields = {}
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–≤–µ—á–µ–π
        ohlc_match = re.search(r'o:([0-9.]+)\|h:([0-9.]+)\|l:([0-9.]+)\|c:([0-9.]+)', line)
        if ohlc_match:
            fields['open'] = float(ohlc_match.group(1))
            fields['high'] = float(ohlc_match.group(2))
            fields['low'] = float(ohlc_match.group(3))
            fields['close'] = float(ohlc_match.group(4))
        
        volume_match = re.search(r'\|([0-9.]+)K\|', line)
        if volume_match:
            fields['volume'] = float(volume_match.group(1))
        
        range_match = re.search(r'rng:([0-9.]+)', line)
        if range_match:
            fields['range'] = float(range_match.group(1))
        
        color_match = re.search(r'\|(RED|GREEN)\|', line)
        if color_match:
            fields['candle_color'] = color_match.group(1)
        
        change_match = re.search(r'\|(RED|GREEN)\|(-?[0-9.]+)%\|', line)
        if change_match:
            fields['price_change_pct'] = float(change_match.group(2))
        
        # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –í–°–ï–• –ø–æ–ª–µ–π –¥–∞–Ω–Ω—ã—Ö
        universal_pattern = r'([a-zA-Z]+)(\d+|1h|4h|1d|1w)-((?:!+)|(?:--?\d+(?:\.\d+)?(?:%)?)|(?:-?\d+(?:\.\d+)?(?:%)?))'
        
        for match in re.finditer(universal_pattern, line):
            prefix = match.group(1)
            suffix = match.group(2)
            value = match.group(3)
            field_name = f"{prefix}{suffix}"
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–Ω–∞—á–µ–Ω–∏–π
            if '!' in value:
                # –°–∏–≥–Ω–∞–ª—ã —Ç–∏–ø–∞ !!, !!!
                fields[field_name] = len(value)
                fields[f"{field_name}_signal"] = value
            elif '%' in value:
                # –ü—Ä–æ—Ü–µ–Ω—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                num_value = float(value.replace('%', ''))
                if value.startswith('--'):
                    fields[field_name] = -num_value
                else:
                    fields[field_name] = num_value
            else:
                # –ß–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                if value.startswith('--'):
                    fields[field_name] = -float(value[2:])
                elif value.startswith('-'):
                    fields[field_name] = -float(value[1:])
                else:
                    fields[field_name] = float(value)
        
        return fields
    
    def load_and_parse_data(self, file_path: str) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö"""
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()
        
        print(f"üìã –ù–∞–π–¥–µ–Ω–æ {len(lines)} —Å—Ç—Ä–æ–∫")
        
        for i, line in enumerate(lines):
            line = line.strip()
            if not line or line.startswith('#'):
                continue
            
            try:
                fields = self.parse_line_all_fields(line)
                if fields and 'close' in fields:
                    fields['line_number'] = i
                    fields['timestamp'] = self._extract_timestamp(line)
                    self.all_data.append(fields)
                    
                if (i + 1) % 1000 == 0:
                    print(f"   –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {i + 1}/{len(lines)} —Å—Ç—Ä–æ–∫")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –≤ —Å—Ç—Ä–æ–∫–µ {i}: {str(e)[:100]}")
                continue
        
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.all_data)} –∑–∞–ø–∏—Å–µ–π")
        
        # –°–æ–∑–¥–∞–µ–º DataFrame
        self.df = pd.DataFrame(self.all_data)
        self.df = self.df.sort_values('line_number').reset_index(drop=True)
        
        print(f"üìä –°–æ–∑–¥–∞–Ω DataFrame: {len(self.df)} —Å—Ç—Ä–æ–∫ √ó {len(self.df.columns)} —Å—Ç–æ–ª–±—Ü–æ–≤")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–æ–ª—è–º
        self._analyze_field_coverage()
    
    def _extract_timestamp(self, line: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ timestamp"""
        ts_match = re.search(r'\[([^\]]+)\]', line)
        return ts_match.group(1) if ts_match else None
    
    def _analyze_field_coverage(self) -> None:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è –ø–æ–ª–µ–π"""
        print("\nüìä –ê–ù–ê–õ–ò–ó –ü–û–ö–†–´–¢–ò–Ø –ü–û–õ–ï–ô:")
        
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–æ–ª—è
        exclude_fields = {'line_number', 'timestamp', 'open', 'high', 'low', 'close', 
                         'volume', 'range', 'candle_color', 'price_change_pct'}
        
        indicator_fields = [col for col in self.df.columns 
                           if col not in exclude_fields and not col.endswith('_signal')]
        
        print(f"   –í—Å–µ–≥–æ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ª–µ–π: {len(indicator_fields)}")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ—Ñ–∏–∫—Å–∞–º
        field_groups = defaultdict(list)
        for field in indicator_fields:
            prefix = re.match(r'^([a-zA-Z]+)', field)
            if prefix:
                field_groups[prefix.group(1)].append(field)
        
        print(f"   –ì—Ä—É–ø–ø –ø–æ–ª–µ–π: {len(field_groups)}")
        for prefix, fields in sorted(field_groups.items()):
            coverage = sum(self.df[field].notna().sum() for field in fields)
            print(f"     {prefix}: {len(fields)} –ø–æ–ª–µ–π, {coverage} –∞–∫—Ç–∏–≤–∞—Ü–∏–π")
    
    def detect_extrema_events(self) -> List[Dict]:
        """–ü–æ–∏—Å–∫ —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤ –∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π"""
        print("\nüéØ –ü–û–ò–°–ö –≠–ö–°–¢–†–ï–ú–£–ú–û–í –ò –°–û–ë–´–¢–ò–ô...")
        
        if len(self.df) < 50:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return []
        
        events = []
        
        # –°–∫–æ–ª—å–∑—è—â–∏–µ –æ–∫–Ω–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –ª–æ–∫–∞–ª—å–Ω—ã—Ö —ç–∫—Å—Ç—Ä–µ–º—É–º–æ–≤
        window = 10
        
        # –ù–∞—Ö–æ–¥–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã
        for i in range(window, len(self.df) - window - 30):  # –û—Å—Ç–∞–≤–ª—è–µ–º 30 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –æ—Ç–∫–∞—Ç–æ–≤
            current_low = self.df.iloc[i]['low']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º
            is_local_min = True
            for j in range(i - window, i + window + 1):
                if j != i and self.df.iloc[j]['low'] <= current_low:
                    is_local_min = False
                    break
            
            if is_local_min:
                event = self._analyze_low_event(i)
                if event:
                    events.append(event)
        
        # –ù–∞—Ö–æ–¥–∏–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã
        for i in range(window, len(self.df) - window - 30):
            current_high = self.df.iloc[i]['high']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
            is_local_max = True
            for j in range(i - window, i + window + 1):
                if j != i and self.df.iloc[j]['high'] >= current_high:
                    is_local_max = False
                    break
            
            if is_local_max:
                event = self._analyze_high_event(i)
                if event:
                    events.append(event)
        
        self.events = events
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(events)} —Å–æ–±—ã—Ç–∏–π")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        event_types = defaultdict(int)
        for event in events:
            event_types[event['type']] += 1
        
        print("üìà –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π:")
        for event_type, count in sorted(event_types.items()):
            print(f"   {event_type}: {count}")
        
        return events
    
    def _analyze_low_event(self, idx: int) -> Optional[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏—è –ª–æ–∏"""
        low_price = self.df.iloc[idx]['low']
        
        # –°–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â–∏–µ 30 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–∫–∞—Ç–∞
        future_data = self.df.iloc[idx:idx+30]
        if len(future_data) < 10:
            return None
        
        max_high_after = future_data['high'].max()
        rebound_pct = ((max_high_after - low_price) / low_price) * 100
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è
        if rebound_pct >= 3.0:
            event_type = "low_with_rebound_3pct"
        elif rebound_pct >= 2.0:
            event_type = "low_with_rebound_2pct"
        elif rebound_pct >= 1.0:
            event_type = "low_with_rebound_1pct"
        else:
            event_type = "low_no_rebound"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–æ–±—ã—Ç–∏—è
        indicators = self._extract_all_indicators_at_moment(idx)
        
        return {
            'type': event_type,
            'category': 'low_event',
            'index': idx,
            'timestamp': self.df.iloc[idx].get('timestamp', ''),
            'price': low_price,
            'rebound_pct': rebound_pct,
            'max_price_after': max_high_after,
            'indicators': indicators
        }
    
    def _analyze_high_event(self, idx: int) -> Optional[Dict]:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ–±—ã—Ç–∏—è —Ö–∞–∏"""
        high_price = self.df.iloc[idx]['high']
        
        # –°–º–æ—Ç—Ä–∏–º —Å–ª–µ–¥—É—é—â–∏–µ 30 –∑–∞–ø–∏—Å–µ–π –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ—Ç–∫–∞—Ç–∞
        future_data = self.df.iloc[idx:idx+30]
        if len(future_data) < 10:
            return None
        
        min_low_after = future_data['low'].min()
        decline_pct = ((high_price - min_low_after) / high_price) * 100
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–æ–±—ã—Ç–∏—è
        if decline_pct >= 3.0:
            event_type = "high_with_decline_3pct"
        elif decline_pct >= 2.0:
            event_type = "high_with_decline_2pct"
        elif decline_pct >= 1.0:
            event_type = "high_with_decline_1pct"
        else:
            event_type = "high_no_decline"
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –í–°–ï –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–æ–±—ã—Ç–∏—è
        indicators = self._extract_all_indicators_at_moment(idx)
        
        return {
            'type': event_type,
            'category': 'high_event',
            'index': idx,
            'timestamp': self.df.iloc[idx].get('timestamp', ''),
            'price': high_price,
            'decline_pct': decline_pct,
            'min_price_after': min_low_after,
            'indicators': indicators
        }
    
    def _extract_all_indicators_at_moment(self, idx: int) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –í–°–ï–• –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤ –Ω–∞ –º–æ–º–µ–Ω—Ç —Å–æ–±—ã—Ç–∏—è"""
        row = self.df.iloc[idx]
        indicators = {}
        
        # –ò—Å–∫–ª—é—á–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        exclude_fields = {'line_number', 'timestamp', 'open', 'high', 'low', 'close', 
                         'volume', 'range', 'candle_color', 'price_change_pct'}
        
        for field in self.df.columns:
            if field not in exclude_fields and not field.endswith('_signal'):
                value = row[field]
                if pd.notna(value):
                    indicators[field] = value
        
        return indicators
    
    def analyze_field_correlations(self) -> Dict:
        """DATA-DRIVEN –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –ø–æ–ª–µ–π —Å —Ç–∏–ø–∞–º–∏ —Å–æ–±—ã—Ç–∏–π"""
        print("\nüß† DATA-DRIVEN –ê–ù–ê–õ–ò–ó –ö–û–†–†–ï–õ–Ø–¶–ò–ô...")
        
        if not self.events:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –Ω–∞–π–¥–∏—Ç–µ —Å–æ–±—ã—Ç–∏—è")
            return {}
        
        correlations = {}
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º —Å–æ–±—ã—Ç–∏—è –ø–æ —Ç–∏–ø–∞–º
        events_by_type = defaultdict(list)
        for event in self.events:
            events_by_type[event['type']].append(event)
        
        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º {len(events_by_type)} —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π...")
        
        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Å–æ–±—ã—Ç–∏—è
        for event_type, events_list in events_by_type.items():
            if len(events_list) < 3:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                continue
            
            print(f"\nüìä –ê–Ω–∞–ª–∏–∑: {event_type} ({len(events_list)} —Å–æ–±—ã—Ç–∏–π)")
            
            # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –≤—Å–µ–º –ø–æ–ª—è–º
            field_stats = self._calculate_field_statistics_for_events(events_list)
            correlations[event_type] = field_stats
        
        self.field_correlations = correlations
        return correlations
    
    def _calculate_field_statistics_for_events(self, events_list: List[Dict]) -> Dict:
        """–†–∞—Å—á–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –ø–æ–ª–µ–π –¥–ª—è —Å–ø–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π"""
        all_fields = set()
        for event in events_list:
            all_fields.update(event['indicators'].keys())
        
        field_stats = {}
        
        for field in all_fields:
            values = []
            for event in events_list:
                if field in event['indicators']:
                    value = event['indicators'][field]
                    if isinstance(value, (int, float)) and not isinstance(value, bool):
                        values.append(value)
            
            if len(values) >= 2:  # –ú–∏–Ω–∏–º—É–º –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
                field_stats[field] = {
                    'activation_count': len(values),
                    'activation_rate': len(values) / len(events_list),
                    'mean': np.mean(values),
                    'median': np.median(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'abs_mean': np.mean(np.abs(values))  # –î–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
                }
        
        return field_stats
    
    def find_discriminative_fields(self) -> Dict:
        """–ü–æ–∏—Å–∫ –ø–æ–ª–µ–π —Ä–∞–∑–ª–∏—á–∞—é—â–∏—Ö —Ç–∏–ø—ã —Å–æ–±—ã—Ç–∏–π"""
        print("\nüîç –ü–û–ò–°–ö –î–ò–°–ö–†–ò–ú–ò–ù–ê–¢–ò–í–ù–´–• –ü–û–õ–ï–ô...")
        
        if not self.field_correlations:
            print("‚ùå –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π")
            return {}
        
        discriminative = {}
        
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä—ã —Ç–∏–ø–æ–≤ —Å–æ–±—ã—Ç–∏–π
        comparison_pairs = [
            ("low_with_rebound_3pct", "low_no_rebound"),
            ("high_with_decline_3pct", "high_no_decline"),
            ("low_with_rebound_3pct", "high_with_decline_3pct")
        ]
        
        for type1, type2 in comparison_pairs:
            if type1 in self.field_correlations and type2 in self.field_correlations:
                diff = self._compare_field_patterns(type1, type2)
                if diff:
                    discriminative[f"{type1}_vs_{type2}"] = diff
        
        return discriminative
    
    def _compare_field_patterns(self, type1: str, type2: str) -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –ø–æ–ª–µ–π –º–µ–∂–¥—É —Ç–∏–ø–∞–º–∏ —Å–æ–±—ã—Ç–∏–π"""
        stats1 = self.field_correlations[type1]
        stats2 = self.field_correlations[type2]
        
        differences = {}
        common_fields = set(stats1.keys()) & set(stats2.keys())
        
        for field in common_fields:
            s1 = stats1[field]
            s2 = stats2[field]
            
            # –°–∏–ª–∞ —Ä–∞–∑–ª–∏—á–∏—è
            mean_diff = abs(s1['mean'] - s2['mean'])
            activation_diff = abs(s1['activation_rate'] - s2['activation_rate'])
            
            # –ü–æ–∫–∞–∑–∞—Ç–µ–ª—å –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
            discriminative_power = mean_diff + activation_diff * 10
            
            if discriminative_power > 0.5:
                differences[field] = {
                    'discriminative_power': discriminative_power,
                    'mean_diff': mean_diff,
                    'activation_diff': activation_diff,
                    f'{type1}_mean': s1['mean'],
                    f'{type2}_mean': s2['mean'],
                    f'{type1}_activation': s1['activation_rate'],
                    f'{type2}_activation': s2['activation_rate']
                }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–æ–π —Å–∏–ª–µ
        return dict(sorted(differences.items(), 
                          key=lambda x: x[1]['discriminative_power'], 
                          reverse=True))
    
    def generate_data_driven_tables(self) -> Dict:
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∞–Ω–Ω—ã—Ö"""
        print("\nüìã –°–û–ó–î–ê–ù–ò–ï DATA-DRIVEN –¢–ê–ë–õ–ò–¶...")
        
        if not self.field_correlations:
            return {}
        
        tables = {}
        
        for event_type, field_stats in self.field_correlations.items():
            # –†–∞–Ω–∂–∏—Ä—É–µ–º –ø–æ–ª—è –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            ranked_fields = []
            for field, stats in field_stats.items():
                importance = stats['activation_rate'] * (1 + stats['abs_mean'])
                ranked_fields.append({
                    'field': field,
                    'importance': importance,
                    'activation_rate': stats['activation_rate'],
                    'activation_count': stats['activation_count'],
                    'mean': stats['mean'],
                    'abs_mean': stats['abs_mean']
                })
            
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏
            ranked_fields.sort(key=lambda x: x['importance'], reverse=True)
            tables[event_type] = ranked_fields[:30]  # –¢–æ–ø 30
        
        return tables
    
    def save_results(self, output_dir: str = "results/true_data_driven"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_path}")
        
        # –°–æ–±—ã—Ç–∏—è
        if self.events:
            events_df = pd.DataFrame([
                {
                    'type': event['type'],
                    'category': event['category'],
                    'timestamp': event['timestamp'],
                    'price': event['price'],
                    'change_pct': event.get('rebound_pct', event.get('decline_pct', 0)),
                    'indicator_count': len(event['indicators'])
                }
                for event in self.events
            ])
            events_df.to_csv(output_path / "events_summary.csv", index=False)
        
        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –ø–æ–ª–µ–π
        if self.field_correlations:
            with open(output_path / "field_correlations.json", 'w', encoding='utf-8') as f:
                json.dump(self.field_correlations, f, indent=2, ensure_ascii=False, default=str)
        
        # Data-driven —Ç–∞–±–ª–∏—Ü—ã
        tables = self.generate_data_driven_tables()
        if tables:
            with open(output_path / "data_driven_tables.json", 'w', encoding='utf-8') as f:
                json.dump(tables, f, indent=2, ensure_ascii=False, default=str)
        
        # –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—è
        discriminative = self.find_discriminative_fields()
        if discriminative:
            with open(output_path / "discriminative_fields.json", 'w', encoding='utf-8') as f:
                json.dump(discriminative, f, indent=2, ensure_ascii=False, default=str)
        
        # –ü–æ–Ω—è—Ç–Ω—ã–π –æ—Ç—á–µ—Ç
        self._create_readable_report(output_path, tables, discriminative)
        
        return output_path
    
    def _create_readable_report(self, output_path: Path, tables: Dict, discriminative: Dict):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–Ω—è—Ç–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        report_lines = [
            "# –ü–†–ê–í–ò–õ–¨–ù–´–ô DATA-DRIVEN –ê–ù–ê–õ–ò–ó –î–ê–ú–ü/–ü–ê–ú–ü",
            "=" * 60,
            "",
            f"–î–∞—Ç–∞ –∞–Ω–∞–ª–∏–∑–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(self.df)}",
            f"–ù–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(self.events)}",
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–ª–µ–π: {len(self.df.columns)}",
            "",
            "## –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–û–ë–´–¢–ò–ô",
            ""
        ]
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–∏–ø–∞–º
        if self.events:
            event_counts = defaultdict(int)
            for event in self.events:
                event_counts[event['type']] += 1
            
            for event_type, count in sorted(event_counts.items()):
                report_lines.append(f"{event_type}: {count} —Å–æ–±—ã—Ç–∏–π")
        
        # –¢–æ–ø –ø–æ–ª—è –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π
        if tables:
            for event_type, fields in tables.items():
                report_lines.extend([
                    "",
                    f"## –¢–û–ü –ü–û–õ–Ø –î–õ–Ø {event_type.upper()}",
                    f"(–Ω–∞ –æ—Å–Ω–æ–≤–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ {len([e for e in self.events if e['type'] == event_type])} —Å–æ–±—ã—Ç–∏–π)",
                    ""
                ])
                
                for i, field_data in enumerate(fields[:15], 1):
                    field = field_data['field']
                    importance = field_data['importance']
                    activation = field_data['activation_rate']
                    mean = field_data['mean']
                    count = field_data['activation_count']
                    
                    report_lines.append(
                        f"{i:2d}. {field:20s} | "
                        f"–≤–∞–∂–Ω–æ—Å—Ç—å: {importance:8.2f} | "
                        f"–∞–∫—Ç–∏–≤–∞—Ü–∏—è: {activation:5.1%} | "
                        f"—Å—Ä–µ–¥–Ω–µ–µ: {mean:8.2f} | "
                        f"—Å–æ–±—ã—Ç–∏–π: {count}"
                    )
        
        # –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–µ –ø–æ–ª—è
        if discriminative:
            report_lines.extend([
                "",
                "## –ü–û–õ–Ø –†–ê–ó–õ–ò–ß–ê–Æ–©–ò–ï –¢–ò–ü–´ –°–û–ë–´–¢–ò–ô",
                "(—Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –∑–Ω–∞—á–∏–º—ã–µ —Ä–∞–∑–ª–∏—á–∏—è)",
                ""
            ])
            
            for comparison, fields in discriminative.items():
                report_lines.append(f"### {comparison}")
                
                for field, data in list(fields.items())[:10]:
                    power = data['discriminative_power']
                    report_lines.append(
                        f"  {field}: –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω–∞—è —Å–∏–ª–∞ = {power:.2f}"
                    )
                report_lines.append("")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç—á–µ—Ç
        with open(output_path / "data_driven_report.txt", 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print("   ‚úÖ –°–æ–∑–¥–∞–Ω data-driven –æ—Ç—á–µ—Ç")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ –ü–†–ê–í–ò–õ–¨–ù–´–ô DATA-DRIVEN –ê–ù–ê–õ–ò–ó –î–ê–ú–ü/–ü–ê–ú–ü")
    print("=" * 80)
    print("–ë–ï–ó –ü–†–ï–î–í–ó–Ø–¢–û–°–¢–ò! –¢–û–õ–¨–ö–û –°–¢–ê–¢–ò–°–¢–ò–ö–ê!")
    print("=" * 80)
    
    analyzer = TrueDataDrivenAnalyzer()
    
    try:
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        print("\n1Ô∏è‚É£ –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–•")
        analyzer.load_and_parse_data("data/dslog_btc_0508240229_ltf.txt")
        
        # 2. –ù–∞—Ö–æ–¥–∏–º —Å–æ–±—ã—Ç–∏—è
        print("\n2Ô∏è‚É£ –ü–û–ò–°–ö –°–û–ë–´–¢–ò–ô")
        events = analyzer.detect_extrema_events()
        
        if not events:
            print("‚ùå –°–æ–±—ã—Ç–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        # 3. –ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        print("\n3Ô∏è‚É£ DATA-DRIVEN –ê–ù–ê–õ–ò–ó –ö–û–†–†–ï–õ–Ø–¶–ò–ô")
        correlations = analyzer.analyze_field_correlations()
        
        # 4. –î–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        print("\n4Ô∏è‚É£ –ü–û–ò–°–ö –î–ò–°–ö–†–ò–ú–ò–ù–ê–¢–ò–í–ù–´–• –ü–û–õ–ï–ô")
        discriminative = analyzer.find_discriminative_fields()
        
        # 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        print("\n5Ô∏è‚É£ –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
        output_path = analyzer.save_results()
        
        print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìÅ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {output_path}")
        print(f"üìã –û—Å–Ω–æ–≤–Ω–æ–π –æ—Ç—á–µ—Ç: {output_path}/data_driven_report.txt")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
